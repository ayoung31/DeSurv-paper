---
header-includes:
- \providecommand{\pandocbounded}[1]{#1}
- \usepackage{booktabs}
- \usepackage{float}
- \usepackage{graphicx}
- \usepackage{amsmath}
- \usepackage{amsfonts}
- \usepackage{amssymb}
- \usepackage{bbm}
- \usepackage{algorithm}
- \usepackage{algpseudocode}
- \usepackage{setspace}
- \DeclareMathOperator*{\argmin}{argmin}
- \DeclareMathOperator*{\argmax}{argmax}
- \DeclareMathOperator\diag{diag}
- \renewcommand{\algorithmicrequire}{\textbf{Input:}}
- \renewcommand{\algorithmicensure}{\textbf{Output:}}
output: pdf_document
---



## Supervision on \(W\) versus \(H\)

Classical nonnegative matrix factorization (NMF) is non-identifiable: for any
invertible matrix \(R\) with nonnegative entries, the factorization
\((W, H)\) can be transformed to \((W R, R^{-1} H)\) without changing the
reconstruction error, yielding multiple valid solutions
[@donoho2004nmf; @laurberg2008uniqueness; @gillis2014nmf]. Although normalizing
columns of \(W\) or rows of \(H\) resolves the trivial scaling ambiguity,
nonnegative mixing transformations persist, and empirical studies consistently
show that the sample-loading matrix \(H\) is more sensitive to initialization
and local minima than the program matrix \(W\)
[@brunet2004metagenes; @kim2007sparse; @gillis2012accelerated].

In DeSurv, survival supervision enters through the projection
\[
Z = X^\top W, \qquad \ell_{\mathrm{Cox}} = \ell_{\mathrm{Cox}}(Z),
\]
so that the supervised gradient with respect to the programs is
\[
\frac{\partial \ell_{\mathrm{Cox}}}{\partial W}
= X\,\frac{\partial \ell_{\mathrm{Cox}}}{\partial Z},
\]
which directly reshapes the gene-program definitions. This mechanism amplifies
genes aligned with the hazard gradient and suppresses those that dilute
prognostic structure, ensuring that the learned programs encode survival-relevant
biological variation.

By contrast, if supervision were applied to the sample loadings \(H\), the model
could reduce the Cox loss by redistributing patient-specific coefficients while
leaving the gene-level programs \(W\) nearly unchanged, a behavior documented in
multiple supervised variants of NMF and supervised topic models, where
supervision applied only to the coefficient matrix modifies \(H\) but not the
basis \(W\) [@cai2011graph; @wang2014supervised; @blei2007slda].

Supervising \(W\) also provides a practical advantage for **portability**:
because \(W\) defines gene-level programs, new samples can be embedded by the
closed-form projection \(Z_{\text{new}} = X_{\text{new}}^\top W\). In contrast,
supervising \(H\) would not yield such a mapping; instead, obtaining sample
loadings for external datasets would require solving a nonnegative least-squares
(NNLS) problem for each new sample, introducing optimization noise and reducing
reproducibility.

Together, these considerations motivate supervising through \(W\), which shapes
the gene programs toward prognostic directions, stabilizes solutions across
initializations, and yields biologically interpretable signatures that
generalize across cohorts.

## Optimization details {.unnumbered}
Algorithm~S\ref{alg:desurv} describes the block coordinate descent (BCD) algorithm for optimizing the DeSurv model in Equation~\ref{eqn:desurv}.


\begin{algorithm}[H]
\caption{DeSurv block coordinate descent}
\label{alg:desurv}
    \begin{algorithmic}[1]
        \Require $X \in \mathbb{R}_{\geq 0}^{p \times n}$, survival times $y \in \mathbb{R}^n_{\geq 0}$, event indicators $\delta \in \{0,1\}^{n}$, tolerance $tol$, maximum iterations $maxit$, supervision parameter $\alpha$, rank $k$, penalties $\lambda_H$, $\lambda$, and $\xi$
        \Ensure Fitted DeSurv parameters $(W,H,\beta)$

        \State Initialize $W^{(0)}, H^{(0)}$ with positive entries (e.g., $W^{(0)},H^{(0)} \sim \mathrm{Uniform}(0,\max X)$)
        \State Initialize $\beta^{(0)}$ (e.g., $\beta^{(0)} = \mathbf{1}$)
        \State $loss = \mathcal{L}(W^{(0)},H^{(0)},\beta^{(0)})$
        \State $eps = \infty$, $t = 0$
        \While{$eps \geq tol$ \textbf{and} $t < maxit$}
            \State \textbf{(H-update)} 
            \State \hspace{0.5cm} Update $H^{(t+1)}$ from $H^{(t)}$ using the multiplicative rule in Eq.~\ref{eqn:Hupdate}, 
            \State \hspace{0.5cm} holding $W^{(t)}$ and $\beta^{(t)}$ fixed.
            \State \textbf{(W-update)} 
            \State \hspace{0.5cm} Update $W^{(t+1)}$ from $W^{(t)}$ using the hybrid multiplicative rule in Eq.~\ref{W_update_convergence},
            \State \hspace{0.5cm} holding $H^{(t+1)}$ and $\beta^{(t)}$ fixed, and perform Armijo backtracking to ensure 
            \State \hspace{0.5cm} $\mathcal{L}$ does not increase.
            \State \textbf{(Column normalization)}
            \State \hspace{0.5cm} Compute $D = \mathrm{diag}\!\big(\lVert W^{(t+1)}_{:,1}\rVert_2,\dots,\lVert W^{(t+1)}_{:,k}\rVert_2\big)$
            \State \hspace{0.5cm} and set $W^{(t+1)} \leftarrow W^{(t+1)} D^{-1}$, $H^{(t+1)} \leftarrow D H^{(t+1)}$, and $\beta^{(t)} \leftarrow D \beta^{(t)}$
            \State \textbf{($\beta$-update)}
            \State \hspace{0.5cm} Update $\beta^{(t+1)}$ from $\beta^{(t)}$ using a Newton-like step for the Cox loss
            \State \hspace{0.5cm} (Eq.~\ref{eqn:beta_update}), holding $W^{(t+1)}$ and $H^{(t+1)}$ fixed.
            \State $lossNew = \mathcal{L}(W^{(t+1)},H^{(t+1)},\beta^{(t+1)})$
            \State $eps = |lossNew - loss| / |loss|$
            \State $loss = lossNew$
            \State $t = t + 1$
        \EndWhile
        \State \Return $\hat{W} = W^{(t+1)}, \hat{H} = H^{(t+1)}, \hat{\beta}=\beta^{(t+1)}$
    \end{algorithmic}
\end{algorithm}

### Update for $H$ {.unnumbered}
We adopt the standard multiplicative update for NMF @Lee1999,
\begin{equation}
    H \leftarrow H \odot \frac{W^\top X}{W^\top W H},
\end{equation}
  which guarantees nonnegativity and monotonic decrease in the reconstruction error conditional on $W$.
  
### Update for $W$ {.unnumbered}
For $W$, we derive a hybrid multiplicative update rule with gradient balancing.
Let,
\begin{equation}
R^{(t)} = \frac{\frac{(1-\alpha)}{np}XH^T + \delta^{(t)} \nabla_W\ell(W^{(t)}, \beta)}{\frac{(1-\alpha)}{np}W^{(t)}HH^T},
\end{equation}
then we let the update for $W$ be
\begin{align}
\label{eqn:W_update}
W^{(t+1)} = W^{(t)} \odot R^{(t)}
\end{align}
where 
\begin{equation}
    \delta^{(t)} =\frac{||\frac{(1-\alpha)}{np}\nabla_W L_{NMF}(W^{(t)},H)||^2_F}{||\alpha\nabla_W L_{cox}(W^{(t)},\beta)||^2_F} =
    \frac{\frac{(1-\alpha)}{np}||\nabla_W L_{NMF}(W^{(t)},H)||^2_F}{\alpha||\nabla_W \ell(W^{(t)},\beta)||^2_F}
\end{equation}
balances the contribution of the NMF and Cox gradients to the update. The quantity $\nabla_W\ell(W^{(t)}, \beta)$ denotes the gradient of the Cox partial log-likelihood with respect to $W$ evaluated at $W^{(t)}$
\begin{equation}
  \nabla_W \ell(W^{(t)}, \beta) = \frac{2}{n_{event}}\sum_{i=1}^n \delta_i \left(x_i - \frac{\sum_{y_j \geq y_i} x_j e^{x_j^TW^{(t)}\beta}}{\sum_{y_j \geq y_i} e^{x_j^TW^{(t)}\beta}}\right) \beta^T,
\end{equation}
and the quantity $\nabla\mathcal{L}_{NMF}(W^{(t)},H)$ is the gradient of the reconstruction error in Equation~\ref{eqn:recon} with respect to $W$ evaluated at $W^{(t)}$
\begin{equation}
    \nabla\mathcal{L}_{NMF}(W^{(t)},H) = 2(W^{(t)}H-X)H^T.
\end{equation}

Note that the update rule in Equation~\ref{eqn:W_update} can also be derive from variable metric projected coordinate descent. See Supplementary Section XX.

Because the update described above does not guarantee a decrease in the overall loss, we employ Armijo backtracking as follows.

\begin{algorithm}[H]
\caption{DeSurv block coordinate descent}
\label{alg:desurv}
    \begin{algorithmic}[1]
        \Require $W^{(t)}$, $R^{(t)}$
        \Ensure $W^{(t+1)}$
        
        \State Compute the overall loss at $W^{(t)}$: $\mathcal{L}(W^{(t)},H^{(t+1)},\beta^{(t)})$
        \State Compute the candidate update for $W$: $W^{(t+1)}_{cand} = W^{(t)} \odot R^{(t)}$
        \State Compute the overall loss at $W^{(t+1)}_{cand}$: $\mathcal{L}(W^{(t+1)}_{cand},H^{(t+1)},\beta^{(t)})$
        \While{$\mathcal{L}(W^{(t+1)}_{cand},H^{(t+1)},\beta^{(t)})$ > $\mathcal{L}(W^{(t)},H^{(t+1)},\beta^{(t)})$}
            \State Decrease the magnitude of $R^{(t)}$ via: $R^{(t)}_{new} = \tau * R^{(t)}$
            \State Recompute the $W$ update: $W^{(t+1)}_{cand} = W^{(t)} \odot R^{(t)}_{new}$
            \State Recompute the overall loss 
    \end{algorithmic}
\end{algorithm}


### Update for $\beta$ {.unnumbered} 
Conditional on $(W,H)$ we solve the elastic-net penalized Cox model using coordinate descent as derived in [@simon2011].
\begin{equation}
\label{eqn:beta_update}
    \hat{\beta}_r = \frac{S(\frac{1}{n}\sum_{i=1}^n w(\tilde{\eta})_i z_{i,r} \left[ v(\tilde{\eta})_i - \sum_{j\ne r} z_{ij} \beta_j,\right], \lambda\xi)}{\frac{1}{n}\sum_{i=1}^n w(\tilde{\eta})_i z_{i,r}^2 + \lambda(1-\xi)},
\end{equation}
The soft-thresholding operator $S(\cdot,\lambda\xi)$ yields closed-form updates for each coefficient. The quantity $w(\tilde{\eta})_i$ is the $i$th diagonal of the Hessian approximation:
\begin{equation}
    w(\tilde{\eta})_m = \ell^{''}(\tilde{\eta})_{m,m} = \sum_{i \in C_m} \left[\frac{
    e^{\tilde{\eta}_m} \sum_{y_j \geq y_i} e^{\tilde{\eta}_j} - (e^{\tilde{\eta}_k})^2
    }{\left(\sum_{y_j \geq y_i} e^{\tilde{\eta}}_j \right)^2} \right],
\end{equation}
and
\begin{equation}
    z(\tilde{\eta})_m = \tilde{\eta}_m - \frac{\ell^{'}(\tilde{\eta})_m}{\ell^{''}(\tilde{\eta})_{m,m}} = \tilde{\eta}_m + \frac{1}{w(\tilde{\eta})_m} \left[\delta_m - \sum_{i \in C_m} \left( \frac{e^{\tilde{\eta}_m}}{\sum_{y_j \geq y_i e^{\tilde{\eta}_j}}}\right) \right]
\end{equation}
and $C_m$ is the set of unique event times at which observation $m$ is still at risk.

### Normalization of W
Normalization of W is performed in every iteration of the DeSurv algorithm to improve numerical stability and interpretability. Because the NMF scale is non-identifiable, foregoing such normalization could allow $W$ to blow up to very large magnitudes while $\beta$ and $H$ shrink to almost zero. This has potential for issues such as division by very small values. Normalization keeps $W$, $H$, and $\beta$ on reasonable scales. Additionally, without normalization one factor may contain magnitude information, while the other encodes almost all structure. Normalizing columns of $W$ makes each basis vector comparable in magnitude and keeps $H$'s columns interpretable as "weights" or "usages".


## Proof of Convergence to a Stationary Point
To prove convergence of Algorithm~S\ref{alg:desurv} to a stationary point of the DeSurv model defined in Equation~\ref{eqn:desurv}, we rely on the theory of @tseng2001convergence, which states: 

We will likely cite lin 2007 here and mention epsilon floors and adding eps to denom of multiplicative updates


## Derivation of W update {.unnumbered}
The W update can be derived directly from the projected coordinate descent update with a specific step size.

Recall that the overall loss function is
\begin{align}
    \mathcal{L}(W,H,\beta) = \frac{(1-\alpha)}{2np} ||X - WH||^2_F - \frac{2 \alpha}{n_{event}} \ell(W,\beta) + \lambda(\xi||\beta||_1 + \frac{(1-\xi)}{2},
\end{align}
where $\ell(W,\beta)$ is the log-partial likelihood for the Cox model. Let $\nabla_W \ell$ represent the derivative of $\ell(W,\beta)$ with respect to $W$. Then the derivative of the overall loss with respect to $W$ is

\begin{equation}
  \frac{\partial L}{\partial W} = \frac{(1-\alpha)}{np}\left( WHH^T - XH^T \right) - \frac{2\alpha}{n_{event}} \nabla_W \ell
\end{equation}

Then the gradient descent update rule at iteration $t$ is
\begin{align}
  W^{(t)} &= W^{(t-1)} - \gamma \left( \frac{\partial L}{\partial W} \right) \nonumber \\
          &= W^{(t-1)} - \gamma \left( \frac{(1-\alpha)}{np}\left( WHH^T - XH^T \right) - \frac{2\alpha}{n_{event}} \nabla_W \ell \right)\\
\end{align}
Let the step size $\gamma$ be defined as in CITE:
\begin{equation}
  \gamma = \frac{np}{(1-\alpha)}\frac{W}{WHH^T}
\end{equation}
Then the update becomes
\begin{align}
  W^{(t)} &= W^{(t-1)} -\frac{np}{(1-\alpha)}\frac{W}{WHH^T} \left( \frac{(1-\alpha)}{np}\left( WHH^T - XH^T \right) - \frac{2\alpha}{n_{event}} \nabla_W \ell \right) \nonumber \\
          &= \frac{W}{WHH^T} XH^T + \frac{2\alpha np}{n_{event}(1-\alpha)} \nabla_W \ell \nonumber \\
          &= W \odot \frac{XH^T + \frac{2\alpha np}{n_{event}(1-\alpha)} \nabla_W \ell }{WHH^T} \nonumber \\
          &= W \odot \frac{\frac{(1-\alpha)}{np} XH^T + \frac{2\alpha}{n_{event}} \nabla_W \ell}{\frac{(1-\alpha)}{np} WHH^T}
\end{align}
Finally, projected coordinate descent projects the $W$ update in to the positive space
\begin{equation}
  W^{(t)} =  \max \left(W \odot \frac{\frac{(1-\alpha)}{np} XH^T + \frac{2\alpha}{n_{event}} \nabla_W \ell}{\frac{(1-\alpha)}{np}WHH^T}, 0\right)
\end{equation}
Since $W \geq 0$ this is equivalent to
\begin{equation}
  W^{(t)} =  W \odot \max \left( \frac{\frac{(1-\alpha)}{np} XH^T + \frac{2\alpha}{n_{event}} \nabla_W \ell}{\frac{(1-\alpha)}{np} WHH^T}, 0 \right)
\end{equation}
which matches the multiplicative form used in the software implementation.

## Cross-validation procedure {.unnumbered}
Algorithm~S\ref{alg:cv} describes the cross validation procedure with $F$ folds and $R$ initializations.

\begin{algorithm}[H]
\caption{Cross-validation for DeSurv pipeline}
\label{alg:cv}
\begin{algorithmic}[1]
\Require 
\Statex Number of folds $F$
\Statex Number of initializations $R$
\Statex Observed data ($X$, $y$, $\delta$)
\Statex Hyperparameters $k$, $\alpha$, $\lambda$, $\xi$, and $\lambda_H$
\Ensure The cross-validated c-index

\State Divide subjects into $F$ folds.
\For{$f = 1$ to $F$}
    \State Split data into training and validation $(X_{(-f)},y_{(-f)}, \delta_{(-f)})$, and $(X_{(f)},y_{(f)}, \delta_{(f)})$ sets
    \For{$r = 1$ to $R$}
        \State set.seed$(r)$
        \State Apply Algorithm~\ref{alg:desurv} with inputs $(X_{(-f)},y_{(-f)}, \delta_{(-f)})$, and $(k, \alpha, \lambda, \xi, \lambda_H)$
        \State Obtain $\hat{W}$ and $\hat{\beta}$ as output from Algorithm~\ref{alg:desurv}
        \State Compute the estimated linear predictor: 
        $
        \hat{\eta} = X_{(f)}^T\hat{W}\hat{\beta}
        $
        \State Compute c-index, denoted $\hat{c}_{(f)r}$, according to Equation~\ref{eqn:cindex}
    \EndFor
    \State \textbf{end loop over $r$}
    \State Compute average c-index across initializations:
    $
    \hat{c}_{(f)} = \frac{1}{r}\sum_{r=1}^R\hat{c}_{(f)r}
    $
\EndFor
\State \textbf{end loop over $f$}
\State Compute final cross-validated c-index:
$
\hat{c} = \frac{1}{F}\sum_{f=1}^F \hat{c}_{(f)}
$
\State \Return Final estimate $\hat{c}$

\end{algorithmic}
\end{algorithm}


## Bayesian optimization roadmap {.unnumbered}
While the current study relies on exhaustive grid search, the calibration problem is well suited to Bayesian Optimization (BO). In future work we plan to model the mapping from hyperparameters $(k, \\alpha, \\lambda, \\xi)$ to the cross-validated c-index using a Gaussian Process surrogate,
\\[
f(\\lambda) \\sim \\mathcal{GP}(\\mu(\\lambda), k(\\lambda, \\lambda')),
\\]
and select new evaluations via the Expected Improvement acquisition function. Each BO iteration would:

\\begin{enumerate}
  \\item Fit/refresh the surrogate using all previously evaluated configurations and their observed c-indices.
  \\item Maximize the acquisition function to identify the next configuration $\\lambda_{t+1}$.
  \\item Launch the cross-validation target for $\\lambda_{t+1}$ within the `targets` pipeline.
\\end{enumerate}

This strategy reduces the required number of expensive cross-validation runs while retaining the ability to handle mixed discrete and continuous hyperparameters.


