## NMF {.unnumbered}
Let $X \in R^{p \times n}_{\geq 0}$ denote a nonnegative gene expression matrix of $p$ features (genes) across $n$ subjects. The goal of NMF is to approximate $X$ as the product of two low-rank, nonnegative matrices 
\begin{equation}
X \approx WH,
\end{equation}
where $W \in R^{p \times k}_{\geq 0}$ contains the gene weights, and $H \in R^{k \times n}_{\geq 0}$ contains the factor scores for each subject. The number of latent factors $k$ determines the dimensionality of the shared low-rank representation. The NMF loss is defined as the residual sum of squares.

\begin{equation}\label{nmf_loss}
    \mathcal{L}(W,H)_{NMF} = ||X - WH||^2_F.
\end{equation}


## Cox partial log-likelihood {.unnumbered}
To incorporate survival outcomes, let $T_i$ denote the event time and $C_i$ the censoring time for subject $i$. The observed time is $y_i = \min(T_i,C_i)$, and the event indicator is $\delta_i$.
Given that $W$ is shared across datasets, we define a lower dimensional transformation of the data:
\begin{equation}
Z=X^TW \in R^{n \times k},
\end{equation}
where each row $Z_i^T$ represents the factor signature scores for subject $i$. These scores serve as covariates in a Cox proportional hazards model:
\begin{equation}
h_i(t) = h_0(t)\exp(Z_i^T\beta),
\end{equation}
where $h_0(t)$ is the baseline hazard and $\beta \in R^k$ are the factor specific coefficients. The Cox log partial likelihood is then
\begin{equation}\label{loglik}
  \ell(W,\beta) = \sum_{i=1}^n \delta_i \left[Z_i^T\beta - \log\left(\sum_{j:y_j \geq y_i} \exp \left(Z_j^T\beta\right) \right)\right].
\end{equation}

## DeSurv {.unnumbered}
Building on the definitions above, DeSurv integrates the unsupervised NMF reconstruction loss with the supervised Cox partial likelihood into a single joint objective function. Because the NMF aims to minimize reconstruction error, whereas the Cox model aims to maximize partial likelihood, we reformulate the latter as a loss term by taking its negative. Thus, the combined objective to be minimized is:
\begin{equation}\label{loss}
    \mathcal{L}(W,H,\beta) = \frac{(1-\alpha)}{np} \mathcal{L}_{NMF}(X \approx WH) - \frac{2 \alpha}{n_{event}} \mathcal{L}_{cox}(W,\beta),
\end{equation}
where $\mathcal{L}_{cox}(W,\beta)$ is the elastic net penalized partial log-likelihood:
\begin{equation}\label{pen_loglik}
  \mathcal{L}_{cox}(W,\beta) = \ell(W,\beta) + \lambda(\xi||\beta||_1 + \frac{(1-\xi)}{2} ||\beta||^2_2).
\end{equation}

Here $\lambda$ represents the penalty weight on $\beta$ and $\xi$ is the balance parameter between the L1 and L2 penalty terms.

The hyperparameter $\alpha \in [0,1]$ controls the relative contribution of each component. When $\alpha=0$, the model reduces to standard unsupervised NMF, focusing purely on reconstruction. When $\alpha>0$ the model encourages discovery of latent molecular programs that are both biologically coherent and prognostically informative.

## Optimization framework {.unnumbered}
DeSurv is optimized using an alternating minimization scheme (Algorithm \ref{deSurv}) that iteratively updates $W$, $H$, and $\beta$ until convergence. The sub-problems for $H$ and $\beta$ are convex in the corresponding parameter conditional on the others.

\begin{algorithm}
\caption{DeSurv algorithm}\label{deSurv}
    \begin{spacing}{1.2} 
    \begin{algorithmic}[1]
        \Require $X \in \mathbb{R}_{\geq 0}^{p \times n}$, $y \in \mathbb{R}^n_{\geq 0}$, $\delta \in \mathbb{R}_{0,1}^{n}$, $W^{(0)}$, $H^{(0)}$, $\beta^{(0)}$, $tol$, $maxit$
        \State $eps = 0$
        \State $t = 0$
        \State $loss = 0$
        \While{$eps < tol$ \textbf{and} $t < maxit$}
            \State $W^{(t)} = \argmin_{W \geq 0} \mathcal{L}(W,H^{(t-1)},\beta^{(t-1)})$
            \State $H^{(t)} = \argmin_{H \geq 0} \mathcal{L}(W^{(t)},H,\beta^{(t-1)})$
            \State $\beta^{(t)} = \argmin_{\beta} \mathcal{L}(W^{(t)},H^{(t)},\beta)$
            \State $lossNew = \mathcal{L}(W^{(t)},H^{(t)},\beta^{(t)})$
            \State $eps = |lossNew - loss|/loss$
            \State $loss = lossNew$
            \State $t = t + 1$
        \EndWhile
        \Return $W$,$H$,$\beta$
    \end{algorithmic}
    \end{spacing}
\end{algorithm}

### Update for $H$ {.unnumbered}
The nonnegative factor matrix $H$ is updated using standard multiplicative updates that guarantee nonnegativity and monotonic decrease in reconstruction error as derived in @Lee1999:
\begin{equation}
\label{H_update}
    H_{ij} = H_{ij} \frac{(W^TX)_{ij}}{(W^TWH)_{ij}}.
\end{equation}

### Update for $\beta$ {.unnumbered}
Given $W$, the coefficients $\beta$ are updated by coordinate descent using elastic-net regularization:
\begin{equation}
\label{beta_update}
    \hat{\beta}_r = \frac{S(\frac{1}{n}\sum_{i=1}^n w(\tilde{\eta})_i v_{i,r} \left[ z(\tilde{\eta})_i - \sum_{j\ne r} v_{ij} \beta_j,\right], \lambda\xi)}{\frac{1}{n}\sum_{i=1}^n w(\tilde{\eta})_i v_{i,r}^2 + \lambda(1-\xi)},
\end{equation}
where $S(.,\lambda\xi)$ is the soft-thresholding operator. The term $w(\tilde{\eta})_i$ is the $i$th diagonal element of the hessian of the partial log-likelihood and $z(\tilde{\eta})_i$ is an approximation of the Newton-Raphson update for $\tilde{\eta}$. Details can be found in @simon2011regularization. The parameters $(\lambda,\xi)$ control the strength and type of regularization.

### Coupled $W$ update {.unnumbered}
The shared basis $W$ is updated through a hybrid multiplicative rule that incorporates both NMF reconstruction gradients and Cox partial likelihood gradients. This update can also be derived from projected coordinate descent (See Supplemental Methods).
\begin{align}
\label{W_update}
W^{(t+1)} = &W^{(t)} \odot \nonumber \\
&\max \left( \frac{\frac{(1-\alpha)}{np}XH^T + \frac{2\alpha}{N_{event}}\nabla_W\ell(W^{(t)}, \beta)}{\frac{(1-\alpha)}{np}W^{(t)}HH^T }, 0 \right)
\end{align}
The quantity $\nabla_W\ell(W^{(t)}, \beta)$ denotes the gradient of the Cox partial likelihood with respect to $W$ evaluated at $W^{(t)}$. 
\begin{equation}
  \nabla_W \ell(W^{(t)}, \beta) = 
\end{equation}
This update allows the survival signal to propagate into the latent factors while preserving nonnegativity.Backtracking and gradient balancing are used in this update to ensure decrease in the overall loss and avoid one component dominating the update.

## Publicly Available Datasets Preprocessing {.unnumbered}
We trained and validated DeSurv using seven publicly available pancreatic ductal adenocarcinoma (PDAC) transcriptomic datasets spanning both RNA-seq and microarray platforms (Table \@ref(tab:datasets)). The TCGA and CPTAC datasets were used for training the DeSurv model, and the remaining 5 were used as external validation. Samples from all datasets were filtered to include only non-metastatic primary PDAC samples. All datasets were log2 + 1 transformed and filtered to the top highly expressed and variable genes. The training data were rank transformed by subject to mitigate cross dataset batch effects and combined, restricting to genes that were kept in each dataset.


\begin{table*}[t]
\centering
\caption{Publicly available pancreatic ductal adenocarcinoma (PDAC) datasets used for model training and validation. Expression data were rank-transformed across genes within samples to mitigate platform- and scale-related effects.}
\label{tab:datasets}
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Dataset} & \textbf{Platform} & \textbf{Samples (n)} & \textbf{Data type} & \textbf{Reference} \\ 
\midrule
TCGA-PAAD & RNA-seq (Illumina HiSeq) & 144 & Discovery / Training & \citep{Raphael2017integrated} \\
CPTAC-PDAC & RNA-seq (Proteogenomic) & 129 & Validation & \citep{ellis2013clinical} \\
Dijk \textit{et al.} & Microarray (Affymetrix) & 90 & Validation & @Dijk2020unsupervised \\
Moffitt \textit{et al.} & Microarray (Affymetrix) & 123 & Validation & \citep{moffitt2015virtual} \\
PACA-AU (array) & Microarray (Agilent) & 63 & Validation & \citep{zhao2018gene} \\
PACA-AU (RNA-seq) & RNA-seq (Illumina HiSeq) & 52 & Validation & \citep{zhao2018gene} \\
Puleo \textit{et al.} & Microarray (Affymetrix) & 288 & Validation & \citep{puleo2018stratification} \\ 
\bottomrule
\end{tabular}
\end{table*}


## Model Training {.unnumbered}
The TCGA and CPTAC datasets were used for model training. Each dataset was filtered to the top 2000 highly expressed and variable genes. These gene lists were then intersected, resulting in XXX genes incorporated in model training.

## Cross Validation {.unnumbered}
To identify optimal hyperparameters, we employed stratified five-fold cross-validation based on event status. An exhaustive search was performed across a grid of hyperparameters $k = 2,\dots,12$, $\alpha \in \{0.1,0.2,\dots,0.9\}$, $\lambda \in 10^{\{-3,...,3\}}$, and $\xi \in \{0,0.1,0.2,\dots,1\}$. Within each fold, models were fit on 80% of the data and evaluated on the held-out 20%. Because NMF solutions are non-unique and sensitive to initialization, each fold was repeated with 20 random seeds for $W$, $H$, and $\beta$, resulting in a total of 100 trained models per parameter configuration of $k$, $lambda$, and $eta$. Warm-start initializations were used across $alpha$ to accelerate convergence.

Final hyperparameters were selected as the combination that produced average C-index (across initializations and folds) within one standard error of the maximum (1 s.e. rule).

## Standard NMF {.unnumbered}
As an unsupervised baseline, we trained conventional NMF models that minimized only the reconstruction error, corresponding to setting $alpha=0$ in the DeSurv formulation. For each rank $k \in {2,...,12}$, 100 random initializations were performed to address non-uniqueness. The initialization with the smallest reconstruction error for each k was selected. To select the rank, $k$, in the unsupervised setting, we used standard metrics including cophenetic coefficient, dispersion, explained variance, residuals, silhouette score, and sparseness. The resulting gene-factor matrix $W$ and validation data were subsequently used as input to a Cox proportional hazards model to evaluate the prognostic value of the unsupervised factors.

## Factor-specific gene signatures {.unnumbered}
For each factor $f$ and gene $g$, we calculated the difference in weight of gene $g$ in factor $f$ and the maximum weight of gene $g$ across all other factors. We define this difference as $s_{gf}$.
\begin{equation}
s_{gf} = W_{gf} - \max_{r \ne f}(W_{gr})
\end{equation}
We then rank $s_{gf}$ across all genes $g=1,\dots,p$ from largest to smallest. Genes that appear at the top of this list for each factor are considered to be factor-specific.

## Clustering {.unnumbered}
Clustering was performed using the ConsensusClusterPlus package in R @wilkerson2010consensusclusterplus. Each validation dataset was restricted to the top 50 factor-specific genes from all prognostic factors prior to clustering. The number of clusters was selected via inspection of cumulative distribution function (CDF) plots, relative change in area under the CDF curve, consensus heatmaps, and cluster consensus scores (Fig SX). After clustering, clusters were manually aligned across datasets based on expression of the factor-specific signatures in each cluster.

## Survival analysis {.unnumbered}
For pooled survival analysis, patients with available OS time and censoring indicator were included. Overall survival estimates were calculated using the Kaplan-Meier method. Association between overall survival and cluster membership were evaluated via the cox proportional hazards models using the coxph function from the ‘survival’ R package (version 3.2-13), where cluster was considered as a multi-level categorical predictor. The log-rank test was used to evaluate the association between cluster membership and overall survival and derive the p-values. In the pooled analyses, a stratified cox proportional hazards model was utilized, where dataset of origin was used as a stratification factor to account for variation in baseline hazard across studies.
