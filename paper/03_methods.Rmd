## Joint loss function
DeSurv is constructs a joint loss function of NMF reconstruction error and the cox partial likelihood. The contribution of each component to the overall loss is weighted by hyperparameter $\alpha$.

\begin{equation}
    \mathcal{L}(W,H,\beta) = (1-\alpha) \mathcal{L}(W,H)_{NMF} - \alpha \mathcal{L}(W,\beta)_{cox}
\end{equation}

### Reconstruction error
Let X be a matrix of $p$ features and $n$ subjects. Then the NMF portion of the loss is
\begin{equation}
    \mathcal{L}(W,H)_{NMF} = ||X - WH||^2_F
\end{equation}
where $W \in R^{p \times k}$ is the matrix of feature weights and $H \in R^{k \times n}$ is the matrix of subject weights, where $k$ represents the number of latent factors.

### Cox partial likelihood
Let $y_i = \min(T_i,C_i)$ where $T_i$ is the event time and $C_i$ is the censoring time for the $i$th subject; let $\delta_i$ represent the indicator that the event time for the $i$th subject is observed. Since $W$ is not dataset dependent, we take $Z=X^TW \in R^{n \times k}$ to be the covariates passed to the proportional hazards model. This can be interpreted as a transformation of the data matrix $X$ into the lower dimensional space. The log partial likelihood is

\begin{equation}
    \ell(W,\beta) = \sum_{i=1}^n \delta_i \left[Z_i^T\beta - \log\left(\sum_{j=1}^n \exp \left(Z_j^T\beta\right) \mathbbm{1} (y_j \geq y_i) \right)\right]
\end{equation}

## Update Rules {.unnumbered}
The joint loss function in (1) is nonconvex.  Algorithm (1) provides an update alternates between updating $W$, $H$, and $\beta$ until convergence. 
\begin{algorithm}
    \caption{DeSurv algorithm}\label{deSurv}

    \begin{algorithmic}[1]
        \Require $X \in \mathbb{R}_{\geq 0}^{p \times n}$, $y \in \mathbb{R}^n_{\geq 0}$, $\delta \in \mathbb{R}_{0,1}^{n}$
        \State $eps \gets \infty$
        \State $iter \gets 0$
        \State $W_{jr} \sim Unif(0,\max(X))$ for $j=1,\dots,p$ and $r=1,\dots, k$
        \State $H_{ri} \sim Unif(0, \max(X))$ for $r=1,\dots,k$ and $i=1,\dots,n$
        \While{$eps < tol$ \textbf{and} $iter < maxit$}
            \State $W \gets \argmin_{W \geq 0} \mathcal{L}(W,H,\beta)$
            \State $H \gets \argmin_{H \geq 0} \mathcal{L}(W,H,\beta)$
            \State $\beta \gets \argmin_{\beta} \mathcal{L}(W,H,\beta)$
            \State $errNew \gets \mathcal{L}(W,H,\beta)$
            \State $relErr \gets |errNew - err|/err$
            \State $err \gets errNew$
            \State $iter \gets iter + 1$
        \EndWhile
        \State \Return $W, H, \beta$
    \end{algorithmic}
\end{algorithm}

### H update
Since the H update does not depend on $\beta$, a standard multiplicative update can be used
\begin{equation}
    H_{ij} = H_{ij} \frac{(W^TX)_{ij}}{(W^TWH)_{ij}}
\end{equation}

### $\beta$ update
Holding $W$ fixed, the $beta$ update for covariates $Z$ solves the standard convex penalized weighted least squares problem. The update as derived in \cite{simon2011regularization} is

\begin{equation}
    \hat{\beta}_r = \frac{S(\frac{1}{n}\sum_{i=1}^n w(\Tilde{\eta})_i v_{i,r} \left[ z(\Tilde{\eta})_i - \sum_{j\ne r} v_{ij} \beta_j,\right], \lambda\xi)}{\frac{1}{n}\sum_{i=1}^n w(\Tilde{\eta})_i v_{i,r}^2 + \lambda(1-\xi)}
\end{equation}

where describe wtilde and ztilde

\subsection{Publicly Available Datasets}
To train and validate our model we used seven publicly available PDAC datasets. RNAseq datasets were in TPM units. Each dataset was log2 + 1 transformed for variance stabilization. Additionally each dataset was rank transformed to mitigate scale differences between datasets and platforms.
For each include sample size, citation, platform
\begin{itemize}
  \item TCGA 
  \item CPTAC
  \item Dijk
  \item Moffitt
  \item PACA array
  \item PACA seq
  \item Puleo
\end{itemize}

\subsection{Model Training}
The TCGA dataset was used for model training. The data was filtered to the top 1000 highly expressed and variable genes. Models were trained across a grid of hyperparameters $\alpha \in \{0,.95\}$, $\lambda \in$, $\xi \in$, $\gamma \in$, $\nu \in$, and $k = 2,\dots,15$

\subsubsection{Hyperparameter selection}
The hyperparameters $\alpha$, $\lambda$, $\xi$, $\gamma$, and $\nu$ were selected to adequately balance the supervised and unsupervised portions of the model using a metric we defined as the c-index of the proportional hazards model divided by the reconstruction error. The parameters were chosen to maximize this metric. Since the reconstruction error exclusively decreases as the dimension $k$ increases, this metric was not adequate to choose $k$. 

Cross-validation was used to select the optimal hyperparameters. To account for lack of uniqueness of NMF solutions, we used 50 random initializations of W,H, and beta per fold. Validation metrics were averaged across fold and initialization. 

\subsection{Model Validation}
The remaining 7 publicly available PDAC datasets, CPTAC, Dijk, Linehan, Moffitt, PACA microarray, PACA RNAseq, and Puleo, were used to validate our models. The datasets were restricted to the same highly variable genes in the TCGA dataset, and the fitted model was applied to each dataset individually. The partial likelihood and c-index were calculated for each dataset. Hazard ratios were reported for each factor.

\subsection{Score based approach}
In a clinical setting, it may not be feasible to sequence all of the genes required to port the full W matrix to future test sets. For this purpose we also propose a score based method, where only the identity of top genes for each factor must be ported to future datasets. To construct the scores we define Wtilde as a binary matrix... To predict patient outcomes in future datasets, we restrict those datasets to these g x k genes, and compute XtWtilde as linear predictor in the survival model.

\subsubsection{Top genes}
The top genes were extracted from each factor of W in the selected model at each value of $k$. A top gene was defined as ...
