## Joint loss function
DeSurv is constructs a joint loss function of NMF reconstruction error and the cox partial likelihood. The contribution of each component to the overall loss is weighted by hyperparameter $\alpha$.

\begin{equation}
    \mathcal{L}(W,H,\beta) = (1-\alpha) \mathcal{L}(W,H)_{NMF} - \alpha \mathcal{L}(W,\beta)_{cox}
\end{equation}

### Reconstruction error
Let X be a matrix of $p$ features and $n$ subjects. Then the NMF portion of the loss is
\begin{equation}
    \mathcal{L}(W,H)_{NMF} = ||X - WH||^2_F
\end{equation}
where $W \in R^{p \times k}$ is the matrix of feature weights and $H \in R^{k \times n}$ is the matrix of subject weights, where $k$ represents the number of latent factors.

### Cox partial likelihood
Let $y_i = \min(T_i,C_i)$ where $T_i$ is the event time and $C_i$ is the censoring time for the $i$th subject; let $\delta_i$ represent the indicator that the event time for the $i$th subject is observed. Since $W$ is not dataset dependent, we take $Z=X^TW \in R^{n \times k}$ to be the covariates passed to the proportional hazards model. This can be interpreted as a transformation of the data matrix $X$ into the lower dimensional space. The log partial likelihood is

\begin{equation}
    \ell(W,\beta) = \sum_{i=1}^n \delta_i \left[Z_i^T\beta - \log\left(\sum_{j=1}^n \exp \left(Z_j^T\beta\right) \mathbbm{1} (y_j \geq y_i) \right)\right]
\end{equation}

## Update Rules {.unnumbered}
The joint loss function in (1) is nonconvex.  Algorithm (1) provides an update alternates between updating $W$, $H$, and $\beta$ until convergence. 
\begin{algorithm}
    \caption{DeSurv algorithm}\label{deSurv}

    \begin{algorithmic}[1]
        \Require $X \in \mathbb{R}_{\geq 0}^{p \times n}$, $y \in \mathbb{R}^n_{\geq 0}$, $\delta \in \mathbb{R}_{0,1}^{n}$
        \State $eps \gets \infty$
        \State $iter \gets 0$
        \State $W_{jr} \sim Unif(0,\max(X))$ for $j=1,\dots,p$ and $r=1,\dots, k$
        \State $H_{ri} \sim Unif(0, \max(X))$ for $r=1,\dots,k$ and $i=1,\dots,n$
        \While{$eps < tol$ \textbf{and} $iter < maxit$}
            \State $W \gets \argmin_{W \geq 0} \mathcal{L}(W,H,\beta)$
            \State $H \gets \argmin_{H \geq 0} \mathcal{L}(W,H,\beta)$
            \State $\beta \gets \argmin_{\beta} \mathcal{L}(W,H,\beta)$
            \State $errNew \gets \mathcal{L}(W,H,\beta)$
            \State $relErr \gets |errNew - err|/err$
            \State $err \gets errNew$
            \State $iter \gets iter + 1$
        \EndWhile
        \State \Return $W, H, \beta$
    \end{algorithmic}
\end{algorithm}

### H update
Since the H update does not depend on $\beta$, a standard multiplicative update can be used
\begin{equation}
    H_{ij} = H_{ij} \frac{(W^TX)_{ij}}{(W^TWH)_{ij}}
\end{equation}

### $\beta$ update
Holding $W$ fixed, the $beta$ update for covariates $Z$ solves the standard convex penalized weighted least squares problem. The update as derived in \cite{simon2011regularization} is

\begin{equation}
    \hat{\beta}_r = \frac{S(\frac{1}{n}\sum_{i=1}^n w(\Tilde{\eta})_i v_{i,r} \left[ z(\Tilde{\eta})_i - \sum_{j\ne r} v_{ij} \beta_j,\right], \lambda\xi)}{\frac{1}{n}\sum_{i=1}^n w(\Tilde{\eta})_i v_{i,r}^2 + \lambda(1-\xi)}
\end{equation}

where describe wtilde and ztilde

\subsection{Publicly Available Datasets}
To train and validate our model we used seven publicly available PDAC datasets. RNAseq datasets were in TPM units. Each dataset was log2 + 1 transformed for variance stabilization. Additionally each dataset was rank transformed to mitigate scale differences between datasets and platforms.
For each include sample size, citation, platform
\begin{itemize}
  \item TCGA 
  \item CPTAC
  \item Dijk
  \item Moffitt
  \item PACA array
  \item PACA seq
  \item Puleo
\end{itemize}

\subsection{Model Training}
The TCGA dataset was used for model training. The data was filtered to the top 1000 highly expressed and variable genes. Models were trained across a grid of hyperparameters $\alpha \in \{0,.95\}$, $\lambda \in$, $\xi \in$, $\gamma \in$, $\nu \in$, and $k = 2,\dots,15$

\subsubsection{Hyperparameter selection}
The hyperparameters $\alpha$, $\lambda$, $\xi$, $\gamma$, and $\nu$ were selected to adequately balance the supervised and unsupervised portions of the model using a metric we defined as the c-index of the proportional hazards model divided by the reconstruction error. The parameters were chosen to maximize this metric. Since the reconstruction error exclusively decreases as the dimension $k$ increases, this metric was not adequate to choose $k$. 

Cross-validation was used to select the optimal hyperparameters. To account for lack of uniqueness of NMF solutions, we used 50 random initializations of W,H, and beta per fold. Validation metrics were averaged across fold and initialization. 

\subsection{Model Validation}
The remaining 7 publicly available PDAC datasets, CPTAC, Dijk, Linehan, Moffitt, PACA microarray, PACA RNAseq, and Puleo, were used to validate our models. The datasets were restricted to the same highly variable genes in the TCGA dataset, and the fitted model was applied to each dataset individually. The partial likelihood and c-index were calculated for each dataset. Hazard ratios were reported for each factor.

\subsection{Score based approach}
In a clinical setting, it may not be feasible to sequence all of the genes required to port the full W matrix to future test sets. For this purpose we also propose a score based method, where only the identity of top genes for each factor must be ported to future datasets. To construct the scores we define Wtilde as a binary matrix... To predict patient outcomes in future datasets, we restrict those datasets to these g x k genes, and compute XtWtilde as linear predictor in the survival model.

\subsubsection{Top genes}
The top genes were extracted from each factor of W in the selected model at each value of $k$. A top gene was defined as ...




\begin{equation}
    p_{\xi}(\beta) = \xi||\beta||_1 + \frac{(1-\xi)}{2} ||\beta||^2_2
\end{equation}

to be an elastic net penalty on the regression coefficients $\beta$. The L1 component allows factors that are not associated with survival to be shrunk out of the proportional hazards model, while still contributing to the reconstruction of $X$. 


## Standard NMF {.unnumbered}
Let $X$ be a bulk gene expression matrix of $p$ genes by $n$ subjects. Standard NMF seeks to reconstruct X using two nonnegative matrices $W$ and $H$ such that $X = WH$, where $W$ is a $p \times k$ matrix of gene weights and $H$ is a $k \times n$ matrix of sample weights. This is done by minimizing the loss function

\begin{equation}
    ||X - WH||^2_F
\end{equation}

where $F$ represents the Frobenius norm. 

Multiplicative updates were proposed by Lee and Seung with the following update rules @lee2000algorithms:

\begin{equation}
    W_{ij} = \frac{W_{ij} (XH^T)_{ij}}{(WHH^T)_{ij}}
\end{equation}

\begin{equation}
    H_{ij} = H_{ij} \frac{(W^TX)_{ij}}{(W^TWH)_{ij}}
\end{equation}

These updates are alternated until convergence to a stationary point.


## Proportional Hazards {.unnumbered}
To determine how the lower dimensional representation of X is associated with patient survival outcomes, we take $Z=W^TX$ to be the covariates passed to the proportional hazards model. The matrix $Z$ can be interpreted as the transformation of the data matrix $X$ into the lower dimensional space, such that $Z_{ri}$ represents a score for the contribution of factor $r$ to subject $i$.

Let $y_i = \min(T_i,C_i)$ where $T_i$ is the event time and $C_i$ is the censoring time for the $i$th subject; let $\delta_i$ represent the indicator that the event time for the $i$th subject is observed. The the log partial likelihood is

\begin{equation}
    \ell(W,\beta) = \sum_{i=1}^n \delta_i \left[Z_i^T\beta - \log\left(\sum_{j=1}^n \exp \left(Z_j^T\beta\right) \mathbbm{1} (y_j \geq y_i) \right)\right]
\end{equation}




## Update for $W$ {.unnumbered}

To get the update rule for $W$, we must find

\begin{equation}
    \argmin_{W \geq 0} \mathcal{L}(W,H,\beta)
\end{equation}

The derivative of the loss with respect to $W$ is

\begin{equation}
        \frac{\partial \mathcal{L}}{\partial W} = \frac{(1-\alpha)}{s} (WH-X)H^T - \frac{2\alpha}{n}\frac{\partial \ell}{\partial W} + \frac{\gamma}{pk}W
\end{equation}

where $\frac{\partial \ell}{\partial W}$ is the derivative of the partial likelihood with respect to $W$

\begin{equation}
    \frac{\partial \ell}{\partial W} = \sum_{i=1}^n \delta_i \left[X_i - \frac{\sum_{j=1}^n e^{Z_j^T\beta} \mathbbm{1} (y_j \geq y_i)X_j}{\sum_{j=1}^n e^{Z_j^T\beta} \mathbbm{1} (y_j \geq y_i)} \right] \beta^T
\end{equation}

A multiplicative update for W can then be formed as

\begin{equation}
    W = W \odot \max\left(\frac{XH^T + \frac{2 \alpha s}{n(1-\alpha)} \frac{\partial \ell}{\partial W}}{WHH^T + \frac{\gamma s}{pk(1-\alpha)}W}, 0 \right)    
\end{equation}

note that the $\max(*, 0)$ is necessary because the derivative of the partial likelihood is not guaranteed to be nonnegative.


## Update for $H$ {.unnumbered}
The update for $H$ is a standard NMF multiplicative update.

\begin{equation}
        H = H \odot \frac{W^TX}{W^TWH + \frac{\nu s}{nk(1-\alpha)}H}
\end{equation}

## Update for $\beta$ {.unnumbered}

To get the update for $\beta$ we take

\begin{equation}
    \beta = \argmin_\beta \mathcal{L}(W,H,\beta)
\end{equation}

which is equivalent to

\begin{equation}
    \beta = \argmax_\beta \frac{2}{n}\ell(W,\beta) - \lambda p_{\xi}(\beta)
\end{equation}

Note that this is the same form as a standard cox partial likelihood update. So the $\beta$ update as derived in \cite{simon2011regularization} is

\begin{equation}
    \hat{\beta}_r = \frac{S(\frac{1}{n}\sum_{i=1}^n w(\Tilde{\eta})_i v_{i,r} \left[ z(\Tilde{\eta})_i - \sum_{j\ne r} v_{ij} \beta_j,\right], \lambda\xi)}{\frac{1}{n}\sum_{i=1}^n w(\Tilde{\eta})_i v_{i,r}^2 + \lambda(1-\xi)}
\end{equation}

where

\begin{equation}
    S(x,\lambda)=sgn(x)(|x|-\lambda)_+.
\end{equation}

\begin{equation}
w(\tilde{\eta})_{r}=\ell^{\prime \prime}(\tilde{\eta})_{r, r}=\sum_{i \in C_{r}}\left[\frac{e^{\tilde{\eta}_{r}} \sum_{j \in R_{i}} e^{\tilde{\eta}_{j}}-\left(e^{\tilde{\eta}_{r}}\right)^{2}}{\left(\sum_{j \in R_{i}} e^{\tilde{\eta}_{j}}\right)^{2}}\right] 
\end{equation}

\begin{equation}
    z(\tilde{\eta})_{r}=\tilde{\eta}_{r}-\frac{\ell^{\prime}(\tilde{\eta})_{r}}{\ell^{\prime \prime}(\tilde{\eta})_{r, r}}=\tilde{\eta}_{r}+\frac{1}{w(\tilde{\eta})_{r}}\left[\delta_{r}-\sum_{i \in C_{r}}\left(\frac{e^{\tilde{\eta}_{r}}}{\sum_{j \in R_{i}} e^{\tilde{\eta}_{j}}}\right)\right]
\end{equation}








