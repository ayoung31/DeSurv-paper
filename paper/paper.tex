\documentclass[9pt,twocolumn,twoside,]{pnas-new}

% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations
% may affect the guide line number alignment.


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


% Pandoc citation processing
%From Pandoc 3.1.8
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\providecommand{\pandocbounded}[1]{#1}
\usepackage{booktabs}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{setspace}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator\diag{diag}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\templatetype{pnasresearcharticle}  % Choose template

\title{Survival driven deconvolution (DeSurv) reveals prognostic and
interpretable cancer subtypes}

\author[a,1,2]{Amber M. Young}
\author[b]{Alisa Yurovsky}
\author[a]{Didong Li}
\author[a,c]{Naim U. Rashid}

  \affil[a]{University of North Carolina at Chapel Hill, Biostatistics,
Street, City, State, Zip}
  \affil[b]{Stony Brook University, Street, City, State, Zip}


% Please give the surname of the lead author for the running footer
\leadauthor{Anonymous}

% Please add here a significance statement to explain the relevance of your work
\significancestatement{Tumor transcriptomes reflect mixtures of
malignant and microenvironmental cell populations, making it challenging
to identify the molecular programs that truly drive clinical outcomes.
Existing deconvolution and matrix factorization methods discover latent
transcriptional programs but do not ensure that these programs are
prognostic, while supervised extensions optimized for prediction offer
limited biological interpretability. We present DeSurv, a
survival-supervised deconvolution framework that integrates nonnegative
matrix factorization with Cox modeling to jointly learn biologically
coherent gene programs and their associations with patient survival. By
embedding outcome information directly into the discovery process and
performing automatic model selection, DeSurv reveals clinically relevant
transcriptional programs that are reproducible across cohorts. This
approach advances the statistical foundations of tumor deconvolution and
provides a general tool for identifying actionable molecular drivers of
disease progression.}


\authorcontributions{Please provide details of author contributions
here.}

\authordeclaration{Please declare any conflict of interest here.}


\correspondingauthor{\textsuperscript{2} To whom correspondence should
be addressed. E-mail:
\href{mailto:ayoung31@live.unc.edu}{\nolinkurl{ayoung31@live.unc.edu}}}

% Keywords are not mandatory, but authors are strongly encouraged to provide them. If provided, please include two to five keywords, separated by the pipe symbol, e.g:
 \keywords{  one |  two |  optional |  optional |  optional  } 

\begin{abstract}
Molecular subtyping in cancer is an ongoing problem that relies on the
identification of robust and replicable gene signatures. While
transcriptomic profiling has revealed recurrent gene expression patterns
in various types of cancer, the prognostic value of these signatures is
typically evaluated in retrospect. This is due to the reliance on
unsupervised learning methods for identifying cell-type-specific signals
and clustering patients into molecular subtypes. Here we present a
Survival-driven Deconvolution tool (deSurv) that integrates bulk
RNA-sequencing data with patient survival information to identify
cell-type-enriched gene signatures associated with prognosis. Applying
deSurv to various cohorts in pancreatic cancer, we uncover prognostic
and biologically interpretable subtypes that reflect the complex
interactions between stroma, tumor, and immune cells in the tumor
microenvironment. Our approach highlights the value of using patient
outcomes during gene signature discovery.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

% Optional adjustment to line up main text (after abstract) of first page with line numbers, when using both lineno and twocolumn options.
% You should only change this length when you've finalised the article contents.
\verticaladjustment{-2pt}



\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.

\acknow{Please include your acknowledgments here, set in a single
paragraph. Please do not include any acknowledgments in the Supporting
Information, or anywhere else in the manuscript.}

Molecular subtyping has transformed precision oncology by stratifying
patients into biologically and clinically meaningful groups that inform
prognosis and guide therapy (1--5). Subtyping relies on the
identification of robust biological signals that define subtypes such as
transcriptomic signatures. However, the tumor microenvironment (TME)
contains mixtures of diverse cell types such as malignant, stromal,
immune, and endothelial cells, and disentangling tumor specific signals
from this mixture can be challenging. As such, subtyping pipelines
typically rely on the deconvolution of bulk transcriptomic data or
single-cell analysis to discover distinct cell types and their
corresponding signatures. Downstream, the signatures are evaluated for
clinical relevance such as overall survival or response to treatment.

Separating discovery from validation can risk overfitting and limits
biological and clinical generalizability. Identified cell types may
capture dataset-specific noise rather than reproducible biological
signals, undermining their utility in downstream analyses or therapeutic
targeting (6, 7). Moreover, even when discovered cell types are
biologically valid and reproducible, they may not correspond to the
cellular programs most relevant for predicting or influencing clinical
outcomes (8, 9). Therefore, there is a clear need for integrative
methods that jointly uncover biologically meaningful programs while
directly incorporating clinical endpoints to ensure prognostic
relevance.

However, integrating patient outcomes into the discovery phase is not
straightforward with current technology and methodology. Single-cell
transcriptomics can resolve programs at the cellular level, but cohort
sizes are often too small to support survival analyses. In contrast,
large bulk transcriptomic cohorts with clinical annotations are
well-suited for outcome modeling (10, 11), yet deconvolution is needed
to disentangle overlapping cellular signals. Reference-based
deconvolution methods focus on estimating cell-type proportions from
predefined signatures, which limits the utility of these methods for
discovery of novel programs (12).

Nonnegative matrix factorization (NMF) is widely used in cancer genomics
because its nonnegativity constraints produce biologically
interpretable, additive molecular programs (13--16). Although recent
extensions have incorporated supervision into the factorization, most
target regression or classification rather than time-to-event outcomes.
Two studies have proposed survival-aware NMF formulations (17, 18), but
both integrate the survival objective through the sample-specific
loadings rather than the gene-level programs. This design emphasizes
prediction accuracy but limits the model's ability to restructure or
refine the underlying molecular programs, reducing its value for
biological interpretation and subtype discovery, which are core
objectives in cancer transcriptomics. In addition, neither study
provides a principled approach for hyperparameter selection or model
assessment, and convergence properties are only briefly addressed in one
manuscript. Both works remain unpublished and unreviewed, leaving their
methodological robustness and reproducibility uncertain. These gaps
highlight the need for a rigorously formulated, survival-aware
deconvolution method that jointly estimates interpretable molecular
programs and their prognostic relevance.

Here we present DeSurv, a Survival-supervised Deconvolution framework
that integrates non-negative matrix factorization (NMF) with Cox
proportional hazards modelling. In contrast to fully unsupervised
approaches that evaluate survival associations only after the
factorization, and to existing supervised NMF models that link outcomes
to the subject-level factor loadings, DeSurv integrates survival
information directly into the gene signature matrix. This design ensures
that the discovered transcriptional programs are not only biologically
interpretable but also intrinsically aligned with patient outcomes. To
enhance robustness and reproducibility, DeSurv performs automatic
parameter selection via Bayesian optimization, addressing the
quintessential challenge of rank determination in matrix factorization.

By coupling latent program discovery with direct survival supervision,
DeSurv resolves longstanding challenges in disentangling
tumor--microenvironment interactions and aligns molecular heterogeneity
with clinical outcomes. This unified approach represents a
methodological advance in translational cancer genomics and provides a
general framework for deriving actionable insights from high-dimensional
transcriptomic data.

\section*{Results}\label{results}
\addcontentsline{toc}{section}{Results}

\subsection*{Model Overview}\label{model-overview}
\addcontentsline{toc}{subsection}{Model Overview}

We have developed an integrated framework, DeSurv, that couples
Nonnegative Matrix Factorization (NMF) with Cox proportional hazards
regression to identify latent gene-expression programs associated with
patient survival (Figure @ref(fig:fig-schema)). The model takes as input
a bulk expression matrix of \(p\) genes by \(n\) patients
(\(X_{Train}\)) together with corresponding survival times
(\(y_{Train}\)) and censoring indicators (\(\delta_{Train}\)) (Figure
\ref{fig:schema}A).

DeSurv optimizes a joint objective combining the NMF reconstruction loss
and the Cox model's log-partial likelihood, weighted by a supervision
parameter (\(\alpha\)) that determines the relative contribution of each
term (fig.~\ref{fig:schema}B): \begin{align}
  (1-\alpha)\ &\mathcal{L}_{NMF}(X_{Train} \approx WH)\nonumber \\ - \alpha\ &\mathcal{L}_{Cox}(X_{Train}^TW\beta,y_{Train},\delta_{Train})
\end{align} When \(\alpha=0\), the method reduces to standard
unsupervised NMF; when \(\alpha>0\), survival information directly
guides the learned factors toward prognostic structure.

Within this framework, the product (\(X_{Train}^TW\)) represents
patient-level factor scores - the inferred burden of each latent program
across subjects. These factor scores serve as covariates in the Cox
model, and their regression coefficients (\(\beta\)) indicate whether
higher activity of a given program corresponds to improved or reduced
survival.

Model training yields gene weights (\(\hat{W}\)), factor loadings
(\(\hat{H}\)), and Cox coefficients (\(\hat{\beta}\)) (Figure
\ref{fig:schema}C), where the inner dimension (\(k\)) specifies the
number of latent factors. Genes with high gene weights in one factor and
low gene weights in all others define the factor-specific signature
genes (Figure \ref{fig:schema}D). By integrating survival supervision
into the factorization, DeSurv not only reconstructs the underlying
expression structure, preserving biologial interpretability, but also
guides latent factors to be prognostically informative. Subsequent
analyses can therefore focus on the survival-associated gene programs
(Figure \ref{fig:schema}E).

\begin{figure*}[t]

{\centering \includegraphics[width=\textwidth,height=6in]{../figures/model_schematic_final} 

}

\caption{Overview of the DeSurv framework and data-driven model selection. (A) DeSurv integrates nonnegative matrix factorization (NMF) with survival modeling to learn prognostic gene programs from a gene expression matrix $X$. NMF decomposes $X \approx WH$, where $W$ represents gene programs and $H$ sample loadings; the learned programs $W$ are shared with a Cox proportional hazards model that links factor-derived scores $Z = X^\top W$ to survival outcomes via regression coefficients $\beta$. A tuning parameter $\alpha$ controls the balance between unsupervised structure learning ($\alpha = 0$) and supervised survival association ($\alpha = 1$). (B) Model complexity ($k$), supervision strength ($\alpha$), and regularization ($\lambda$) are selected via Bayesian optimization using cross-validated concordance index. \label{fig:schema}}\label{fig:fig-schema}
\end{figure*}

\subsection*{\texorpdfstring{Bayesian optimisation selects the DeSurv
hyperparameters (\(k\),
\(\alpha\))}{Bayesian optimisation selects the DeSurv hyperparameters (k, \textbackslash alpha)}}\label{bayesian-optimisation-selects-the-desurv-hyperparameters-k-alpha}
\addcontentsline{toc}{subsection}{Bayesian optimisation selects the
DeSurv hyperparameters (\(k\), \(\alpha\))}

\subsection{Outcome-guided model selection resolves ambiguity in NMF rank choice}

We examined the problem of selecting the number of latent components
(\(k\)) in nonnegative matrix factorization (NMF) using gene expression
data from pancreatic ductal adenocarcinoma (PDAC) cohorts. These
heterogeneous tumor transcriptomes provide a representative setting in
which to evaluate how commonly used unsupervised rank-selection
heuristics behave in practice.

Across a range of candidate ranks, standard NMF diagnostics yielded
inconsistent guidance (Fig. \ref{fig:bo}A-C). Reconstruction residuals
decreased smoothly with increasing \(k\) and did not exhibit a clear
elbow, a pattern consistent with both relatively small solutions
(\(k \approx 3\)-4) and substantially larger ranks (\(k \approx 6\)-8).
The cophenetic correlation coefficient began to decline at low ranks
(\(k \approx 3\)-4) but continued to fluctuate at higher values without
a distinct transition point. In contrast, mean silhouette width,
evaluated across multiple distance metrics, was highest at very small
ranks (\(k \approx 2\)-3) and decreased monotonically thereafter,
favoring low-dimensional solutions that conflicted with recommendations
based on reconstruction error or cophenetic correlation. Together, these
unsupervised criteria pointed to different and incompatible values of
\(k\), highlighting the ambiguity of rank selection in standard NMF when
applied to PDAC data.

To resolve this ambiguity, we applied DeSurv, which incorporates
survival outcomes directly into the factorization process and evaluates
models using survival-based predictive performance. Using the same PDAC
gene expression data, we assessed model performance across the joint
space of the number of components (\(k\)) and supervision strength
(\(\alpha\)) using cross-validated concordance index (C-index). The
resulting C-index surface summarizes expected predictive performance
across candidate models and enables direct comparison of solutions that
differ in both model complexity and degree of supervision (Fig.
\ref{fig:bo}D).

Model selection was based on standard cross-validation principles.
Rather than selecting the single parameter combination with the highest
predicted C-index, we selected the smallest value of \(k\) whose
predicted performance lay within one standard error of the maximum. This
criterion yielded a stable and parsimonious choice of model rank in the
PDAC data, in contrast to the conflicting recommendations produced by
unsupervised NMF heuristics.

To further evaluate rank recovery under controlled conditions, we
conducted simulation studies in which the true underlying rank was known
(\(k = 3\)). Across repeated simulation replicates, DeSurv consistently
selected the correct rank, producing a concentrated distribution of
selected \(k\) values centered at the true value. In contrast, standard
NMF followed by post hoc Cox modeling (\(\alpha = 0\)) exhibited
substantially greater variability and a systematic tendency toward
under-selection. Together, these results indicate that incorporating
outcome information during model fitting improves the reliability of
rank selection in settings where unsupervised criteria yield conflicting
conclusions.

```\{r fig-bo, fig.width = 6, fig.height= 5.5, fig.cap = ``(A-D)
Analyses based on real pancreatic ductal adenocarcinoma (PDAC) gene
expression data illustrate the ambiguity of rank selection in standard
nonnegative matrix factorization (NMF) and the use of outcome
supervision in DeSurv. (A-C) Commonly used unsupervised heuristics for
selecting the number of components (k) yield inconsistent conclusions.
(A) Reconstruction residuals decrease smoothly with increasing k and do
not exhibit a clear elbow; diminishing returns could be inferred at
intermediate (k \(\\approx\) 3-4) or larger (k \(\\approx\) 6-8) ranks.
(B) The cophenetic correlation coefficient, often used to select the
largest k prior to a marked loss of clustering stability, begins to
decline at low ranks (k \(\\approx\) 3-4) but continues to fluctuate
thereafter, providing no unambiguous selection criterion. (C) Mean
silhouette width across multiple distance metrics is highest at small
ranks (k \(\\approx\) 2-3) and decreases monotonically with increasing
k, favoring lower-dimensional solutions that conflict with the other
criteria. (D) Heatmap of the Gaussian process predicted mean
cross-validated concordance index (C-index) from Bayesian optimization
over the joint space of the number of components (k) and supervision
strength (\(\\alpha\)), computed on the same PDAC data. The predicted
performance surface summarizes survival prediction accuracy across
parameter settings and illustrates how DeSurv uses outcome information
to inform model selection. (E) Results from simulation studies with a
known underlying rank (k = 3) showing the distribution of selected k
values across repeated replicates. DeSurv more consistently recovers the
true rank, yielding a concentrated distribution centered at k = 3,
whereas standard NMF with post hoc Cox modeling (\(\\alpha=0\)) exhibits
greater variability and a tendency toward under-selection.
\textbackslash label\{fig:bo\}'', fig.env='figure*`, fig.pos='t',
out.height= ``5.5in'', out.width=``6in''\}

tar\_load(fig\_bo\_heat\_tcgacptac) tar\_load(fig\_residuals\_tcgacptac)
tar\_load(fig\_cophenetic\_tcgacptac)
tar\_load(fig\_silhouette\_tcgacptac)

\section{upper =
plot\_grid(fig\_bo\_cvk\_tcgacptac,fig\_bo\_cvalpha\_tcgacptac,labels=c(``A'',``B''))}\label{upper-plot_gridfig_bo_cvk_tcgacptacfig_bo_cvalpha_tcgacptaclabelscab}

upper = plot\_grid(fig\_residuals\_tcgacptac,
fig\_cophenetic\_tcgacptac, fig\_silhouette\_tcgacptac, ncol = 3, labels
= c(``A'',``B'',``C''))

k\_hist = plot\_grid(alt\_plots\$k\_hist,NULL,nrow=2,rel\_heights =
c(4,1))

lower = plot\_grid(fig\_bo\_heat\_tcgacptac,
k\_hist,ncol=2,labels=c(``D'',``E''),rel\_widths = c(3,2))

plot\_grid(upper,lower,nrow=2,rel\_heights = c(2,3))

\begin{verbatim}


## DeSurv improves selection of prognostic gene signatures {.unnumbered}
To test whether supervision improves the selection of prognostic gene signatures, we used simulations with known lethal factors and tuned the number of top genes per factor (\texttt{n\_top}) using BO. We compared the supervised DeSurv model to the unsupervised $\alpha=0$ baseline across simulation regimes. Figure \ref{fig:sim}A shows the distribution of test C-index values, while Figure \ref{fig:sim}B reports the precision of recovered prognostic genes (mean across lethal factors). DeSurv consistently yields higher C-index and improved precision, indicating that survival supervision helps focus signatures on truly prognostic genes rather than reconstruction-only structure.
\begin{figure*}[t]

{\centering \includegraphics[width=\textwidth]{/work/users/a/y/ayoung31/DeSurv-paper/paper/paper_files/figure-latex/fig-sim-ntop-scenarios-1} 

}

\caption{Performance comparison between DeSurv and unsupervised NMF ($\alpha = 0$) in simulation. (A) Distribution of cross-validated concordance index shows that DeSurv achieves consistently higher survival prediction performance than the unsupervised baseline. (B) Precision for recovering true underlying gene programs is substantially higher for DeSurv, whereas the unsupervised approach exhibits near-zero precision, indicating poor alignment with the ground-truth factors. \label{fig:sim}}\label{fig:fig-sim-ntop-scenarios}
\end{figure*}



## Survival-informed factorization reorganizes transcriptional structure in pancreatic cancer {.unnumbered}
To assess the biological structure captured by standard nonnegative matrix factorization (NMF) and DeSurv, we examined the overlap between factor-specific gene rankings and established PDAC gene programs (Fig. \ref{fig:bio}A-B). Both approaches recovered recognizable biological signals; however, the organization of these signals across factors differed in ways that reflect the objectives of each method.

In the standard NMF solution, factors largely reflected dominant sources of transcriptional variance. One factor was strongly associated with exocrine-associated expression and opposed immune and stromal signatures, consistent with a bulk composition axis separating normal or differentiated tissue from non-epithelial tumor content. A second factor aligned with classical tumor identity, while a third aggregated immune, fibroblast, and extracellular matrix–related programs into a single microenvironmental component. These patterns indicate that standard NMF captures major biological variation in bulk PDAC expression data, but merges distinct microenvironmental states and allocates substantial model capacity to differentiation-driven signals.

By contrast, DeSurv produced a factorization that emphasized axes of variation more closely aligned with disease aggressiveness. One factor corresponded to classical tumor programs opposing basal-like expression, whereas another isolated an activated microenvironmental state characterized by immune infiltration and stromal remodeling. A third factor captured aggressive tumor-intrinsic programs associated with basal-like biology. Notably, exocrine-associated expression did not dominate any DeSurv factor, suggesting that survival-informed optimization deprioritizes differentiation-related variation that is weakly associated with outcome.

These differences were reflected quantitatively by contrasting the fraction of expression variance explained by each factor with its contribution to survival (Fig. \ref{fig:bio}C). In the standard NMF solution, the factor explaining the largest proportion of transcriptional variance contributed little to survival, consistent with its enrichment for exocrine and composition-driven programs. In contrast, DeSurv concentrated survival signal into a single factor that explained substantially more variation in outcome despite accounting for a smaller fraction of expression variance. The remaining DeSurv factors contributed minimal survival signal, indicating that survival-relevant information was not diffusely distributed across components.

To further characterize how DeSurv reorganizes the transcriptional structure learned by standard NMF, we examined the correspondence between factors derived from the two methods (Fig. \ref{fig:bio}D). Tumor-intrinsic structure was largely preserved, with one DeSurv factor showing strong correspondence to the classical tumor–associated NMF factor. In contrast, the microenvironmental factor identified by standard NMF mapped primarily to a single DeSurv factor enriched for activated immune and stromal programs, indicating that DeSurv refines variance-driven microenvironmental structure into a survival-aligned axis. Notably, the NMF factor dominated by exocrine-associated expression did not correspond strongly to any single DeSurv factor, consistent with the suppression of differentiation- and composition-driven signals observed in survival-informed factorization. Together, these results indicate that DeSurv selectively preserves tumor and microenvironmental structure while reorganizing or deprioritizing axes of variation that contribute little to survival.
\begin{figure*}[t]

{\centering \includegraphics[width=\textwidth,height=4.5in]{/work/users/a/y/ayoung31/DeSurv-paper/paper/paper_files/figure-latex/fig-bio-1} 

}

\caption{Survival-informed factorization reorganizes transcriptional structure relative to variance-driven NMF. (A) Correlation of standard NMF factor gene rankings with established pancreatic ductal adenocarcinoma (PDAC) gene programs. NMF recovers variance-dominant structure, including an exocrine-associated factor opposing immune and stromal signatures, a classical tumor factor, and a composite microenvironmental factor. (B) Corresponding correlations for DeSurv. DeSurv isolates tumor-intrinsic classical and basal-like programs and resolves an activated immune–stromal microenvironment, while exocrine-associated expression does not dominate any factor. Asterisks indicate significant correlations after multiple testing correction. * Indicates an fdr adjusted p-value < 0.1 (C) Fraction of expression variance explained versus survival contribution for each factor, quantified by the change in partial log-likelihood from univariate Cox models. Standard NMF factors explain substantial variance with minimal survival relevance, whereas DeSurv concentrates survival signal into a single factor despite explaining less expression variance. (D) Pairwise correspondence between NMF and DeSurv factors. Tumor-intrinsic structure is preserved across methods, while variance-driven microenvironmental structure is reorganized into a survival-aligned axis; the exocrine-dominated NMF factor shows no strong one-to-one correspondence. \label{fig:bio}}\label{fig:fig-bio}
\end{figure*}


## DeSurv-derived latent structure generalizes to independent datasets {.unnumbered}
To assess generalization of DeSurv latent factors, the gene-level signatures learned in the training cohort were used to compute factor activity scores in independent validation datasets. For each validation sample, factor activity was quantified by applying the learned gene-factor weights to the sample’s gene expression profile ($W^TX$), yielding a continuous score for each factor.

For each method, we focused on the factor showing the largest increase in Cox model log partial likelihood in the training data. Across validation cohorts, this DeSurv-derived factor exhibited consistent survival effects, with hazard ratio estimates showing limited variability and predominantly protective associations (Fig. \ref{fig:extval}A). In contrast, the NMF factor identified by the same criterion showed greater heterogeneity across datasets and weaker survival associations.

When validation samples were pooled and stratified into high- and low-activity groups based on the DeSurv-derived factor, clear separation of survival trajectories was observed (Fig. \ref{fig:extval}B). Applying the same procedure to NMF resulted in weaker survival stratification (Fig. \ref{fig:extval}C).

Together, these results indicate that DeSurv identifies latent factors whose associations with survival are preserved across datasets.
\begin{figure*}[t]

{\centering \includegraphics[width=\textwidth]{/work/users/a/y/ayoung31/DeSurv-paper/paper/paper_files/figure-latex/fig-extval-1} 

}

\caption{Figure X. DeSurv learns prognostic structure that generalizes across independent cohorts. (A) Forest plot summarizing hazard ratios (HRs; 95\% CIs) for the latent factor most predictive of survival in held-out validation datasets. Estimates from DeSurv (blue) are more stable across cohorts than those obtained with standard NMF (red). (B) Kaplan–Meier curves for pooled validation samples stratified into high and low groups based on values of the DeSurv-derived factor providing the strongest survival signal. Median-based stratification yields clear separation of survival trajectories, indicating robust out-of-sample prognostic performance. (C) Corresponding analysis for the NMF-derived factor selected using the same survival-based criterion, showing weaker discrimination between risk groups. \label{fig:extval}}\label{fig:fig-extval}
\end{figure*}


## Bladder cancer analysis {.unnumbered}
We will report a focused bladder cancer analysis here, including a dedicated figure. Placeholder text: summary of bladder cohort preprocessing, DeSurv factor interpretation, and survival stratification results to be added once the bladder figure is finalized.




# Discussion {.unnumbered}


We present DeSurv, a survival-driven deconvolution framework that integrates nonnegative matrix factorization with Cox proportional hazards modeling to uncover latent gene-expression programs associated with patient outcomes. By coupling unsupervised matrix decomposition with direct survival supervision, DeSurv bridges the gap between descriptive molecular subtyping and prognostic modeling. Unlike conventional NMF, which identifies factors that explain transcriptional variance without regard to clinical relevance, DeSurv simultaneously optimizes reconstruction fidelity and survival discrimination, yielding interpretable gene programs that are both biologically coherent and clinically informative.

In pancreatic ductal adenocarcinoma (PDAC), DeSurv identified three major transcriptional programs corresponding to immune/iCAF, normal/exocrine, and basal-like factors. The immune/iCAF factor was associated with prolonged survival and enriched for inflammatory and immune-response pathways, consistent with prior evidence linking tumor-infiltrating immune and inflammatory fibroblast populations to improved outcomes. In contrast, the basal-like factor was strongly associated with poor survival and enriched for epithelial–mesenchymal transition and cell-cycle programs characteristic of aggressive tumor phenotypes. The normal/exocrine factor, while not prognostic, captured background pancreatic tissue signal and contributed to accurate reconstruction, demonstrating that DeSurv can disentangle prognostic sources of variation while preserving biology.

Importantly, DeSurv outperformed unsupervised NMF in cross-validated concordance index, achieving higher predictive accuracy with fewer factors. This suggests that explicitly incorporating survival information regularizes the factorization, producing a more parsimonious representation that focuses on biologically and clinically relevant variation. Moreover, when applied to independent PDAC cohorts—including Dijk, PACA-AU, Moffitt, and Puleo—DeSurv-derived gene signatures stratified patients into three reproducible clusters with clear prognostic differences. These clusters aligned strongly with existing PurIST and DeCAF classifiers, recapitulating the interplay between tumor-intrinsic (Basal versus Classical) and stromal (proCAF versus restCAF/iCAF) subtypes. Notably, survival prediction appeared to depend on the presence or absence of basal and immune/iCAF programs rather than binary subtype identities, indicating that the co-activation or suppression of these programs better captures the biological continuum of PDAC progression.

Beyond PDAC, DeSurv provides a generalizable framework for discovering prognostic transcriptional programs across complex cancers where tumor and stromal signals are interwoven. By integrating survival modeling into matrix factorization, it unifies molecular deconvolution and outcome prediction within a single, interpretable model. This approach complements both single-cell–based tumor–stroma characterization and bulk expression subtyping, offering a scalable route to translate transcriptomic heterogeneity into clinically actionable prognostic insight. Future work will extend DeSurv to multi-omic data and time-varying outcomes, further enhancing its ability to resolve dynamic tumor–microenvironment interactions that shape disease progression and therapy response.

# Materials and methods {.unnumbered}


## Problem formulation and notation
Let $X \in \mathbb{R}^{p \times n}_{\ge 0}$ denote the nonnegative gene expression matrix. DeSurv approximates $X \approx WH$, where $W \in \mathbb{R}^{p\times k}_{\ge0}$ contains nonnegative gene programs and $H \in \mathbb{R}^{k\times n}_{\ge0}$ contains sample-level activations. Additionally, let $y,\ \delta \in \mathbb{R}^n$ represent patient survival times and censoring indicators, respectively. Survival outcomes are modeled through a cox proportional hazards model with covariates $Z = W^\top X$.

## The DeSurv Model
DeSurv integrates Nonnegative Matrix Factorization (NMF) with penalized Cox regression to identify gene programs associated with patient survival. 

The joint objective is  
\begin{equation}
\label{eqn:desurv}
\mathcal{L}(W,H,\beta) =
(1-\alpha)\,\mathcal{L}_{\mathrm{NMF}}(W,H)
- \alpha\,\mathcal{L}_{\mathrm{Cox}}(W,\beta),
\end{equation}
where $\mathcal{L}_{\mathrm{NMF}}(W,H)$ is the NMF reconstruction error and $\mathcal{L}_{\mathrm{Cox}}(W,\beta)$ is the elastic-net penalized partial log-likelihood. Optimization proceeds by alternating updates for $H$, $W$, and $\beta$, using multiplicative rules for $H$ [@lee1999learning], projected gradients for $W$, and coordinate descent for $\beta$. Although non-convex, these updates are shown to converge to a stationary point under mild conditions (SI Appendix). Complete derivations and algorithmic details are provided in the SI Appendix.

## Hyperparameter selection and cross-validation 
Hyperparameters $(k,\alpha,\lambda_H,\lambda,\xi)$ were selected by maximizing the cross-validated C-index using Bayesian optimization. Each fold was trained using multiple random initializations, and fold-level performance was defined as the average C-index across initializations. For stability, we used a consensus-based initialization for the final model, aggregating multiple DeSurv runs into a gene--gene co-occurrence matrix and constructing an initialization $W_0$ from the resulting clusters (SI Appendix). Before validation, each column of $W$ was truncated to its BO-selected number of top genes (details in SI Appendix), denoted $\tilde{W}$. External validation was performed by first projecting new datasets onto the learned programs via $Z = \tilde{W}^\top X_{\text{new}}$ and evaluating survival associations using C-index and log-rank statistics. To evaluate the quality of the DeSurv derived gene signatures for subtyping, the new datasets $X_new$ were clustered on genes in $\tilde{W}$, and survival differences were analyzed for the derived clusters. Further training, validation, and runtime details appear in the SI Appendix.


## Simulation studies
Simulation studies were conducted to assess recovery of prognostic latent structure and survival prediction. Gene expression data were generated from a non-negative factor model $X=WH$, where gene loadings $W$ comprised three gene classes: marker genes, background genes, and noise genes. Marker genes were simulated to load strongly on a single factor and weakly on others, background genes to load strongly across all factors, and noise genes to have uniformly low loadings; each class was generated from a distinct gamma distribution. Sample-level factor activities $H$ were generated from a gamma distribution.

Survival times were generated from an exponential proportional hazards model in which risk depended on marker gene structure through $X^T\tilde{W}$, where $\tilde{W}$ retained marker gene loadings for their corresponding factors and was zero otherwise; censoring times were generated independently from an exponential distribution. Each dataset was analyzed using DeSurv, standard NMF followed by Cox regression on inferred factors, and penalized Cox regression (CoxNet), with all methods tuned using the same cross-validated concordance index. Performance was summarized across repeated simulation replicates.

## Evaluation metrics

## Real-world datasets 
We analyzed publicly available RNA-seq and microarray cohorts of pancreatic ductal adenocarcinoma (PDAC) and bladder cancer with corresponding overall survival outcomes. Gene expression matrices were converted to TPM, log-transformed, and filtered to remove low-expression genes. Survival times and censoring indicators were taken from the associated clinical annotations. Of the seven PDAC cohorts we considered, two were used for training (TCGA and CPTAC) and the rest were used for external validation (Dijk, Moffitt, PACA, Puleo). The bladder cohort was split into training and validation cohorts via a 70/30 split. To harmonize differences in scale across cohorts, filtered gene expression data was within-subject rank transformed before model training. More details about the datasets can be found in the SI Appendix.

## Simulations, Benchmarking, and Availability  
Code and processed data used in this study are available at [repository link].




<!-- Leave these lines as they are at the end of your .Rmd file to ensure placement of methods & acknowledgements sections before the references-->
\showmatmethods
\showacknow



























\subsection{Model details}
Let $X \in \mathbb{R}_{\ge 0}^{p \times n}$ denote the nonnegative gene
expression matrix with $p$ genes and $n$ subjects. DeSurv approximates
$X \approx WH$ with $W \in \mathbb{R}_{\ge 0}^{p \times k}$ and
$H \in \mathbb{R}_{\ge 0}^{k \times n}$, and links the shared program matrix
$W$ to survival via an elastic-net–penalized Cox model.

Let $y \in \mathbb{R}^n_{\ge 0}$ be the observed survival times,
$\delta \in \{0,1\}^n$ the event indicators, and
$Z = X^\top W \in \mathbb{R}^{n \times k}$ the sample-wise program
loadings. We write $Z_i^\top$ for the $i$th row of $Z$, and
$n_{\text{event}} = \sum_{i=1}^n \delta_i$. For convenience we recall the
DeSurv loss from Equation~\ref{eqn:desurv} in the main text and rewrite it as
\begin{align}
  \mathcal{L}(W,H,\beta) &=\nonumber \\
  &(1-\alpha)\left(\frac{1}{2np}\lVert X - WH \rVert_F^2 + \frac{\lambda_H}{2nk} \lVert H \rVert_F^2\right) \nonumber \\
  -&\alpha \left(\frac{2}{n_{event}}\ell(W,\beta) - \lambda \{ \xi \lVert \beta \rVert_1 + \tfrac{1-\xi}{2} \lVert \beta \rVert_2^2 \} \right) \nonumber
\end{align}

where $\lVert \cdot \rVert_F$ is the Frobenius norm and
$\beta \in \mathbb{R}^k$ are the Cox coefficients. The Cox log-partial
likelihood $\ell(W,\beta)$ is
\begin{equation}
\ell(W,\beta)
=
\sum_{i=1}^n
\delta_i
\left[
  Z_i^\top \beta
  -
  \log \left(
    \sum_{j : y_j \ge y_i}
    \exp(Z_j^\top \beta)
  \right)
\right].
\end{equation}
Hyperparameters satisfy $\alpha \in [0,1)$,
$\lambda_H \ge 0$, $\lambda \ge 0$, and $\xi \in [0,1]$.
The constants $1/(2np)$, $2/n_{\text{event}}$, and $1/(2nk)$ are chosen
for numerical convenience and do not affect the minimizer.


\subsection{Optimization Algorithm}

\subsubsection{Block coordinate descent scheme}
Algorithm~\ref{alg:desurv} summarizes the block coordinate descent (BCD)
scheme used to minimize $\mathcal{L}(W,H,\beta)$. At each outer iteration,
we update $H$, then $W$, then $\beta$ while holding the other blocks
fixed.

\begin{algorithm}[H]
\caption{DeSurv block coordinate descent}
\label{alg:desurv}
    \begin{algorithmic}[1]
        \Require $X \in \mathbb{R}_{\geq 0}^{p \times n}$, survival times $y \in \mathbb{R}^n_{\geq 0}$, event indicators $\delta \in \{0,1\}^{n}$, tolerance $tol$, maximum iterations $maxit$, supervision parameter $\alpha$, rank $k$, penalties $\lambda_H$, $\lambda$, and $\xi$
        \Ensure Fitted DeSurv parameters $(W,H,\beta)$

        \State Initialize $W^{(0)}, H^{(0)}$ with positive entries (e.g., $W^{(0)},H^{(0)} \sim \mathrm{Uniform}(0,\max X)$)
        \State Initialize $\beta^{(0)}$ (e.g., $\beta^{(0)} = \mathbf{1}$)
        \State $loss = \mathcal{L}(W^{(0)},H^{(0)},\beta^{(0)})$
        \State $eps = \infty$, $t = 0$
        \While{$eps \geq tol$ \textbf{and} $t < maxit$}
            \State \textbf{(H-update)} 
            \State \hspace{0.5cm} Update $H^{(t+1)}$ from $H^{(t)}$ using the multiplicative rule in Eq.~\ref{eqn:Hupdate}, 
            \State \hspace{0.5cm} holding $W^{(t)}$ and $\beta^{(t)}$ fixed.
            \State \textbf{(W-update)} 
            \State \hspace{0.5cm} Update $W^{(t+1)}$ from $W^{(t)}$ using the hybrid multiplicative rule in Eq.~\ref{eqn:W_update},
            \State \hspace{0.5cm} holding $H^{(t+1)}$ and $\beta^{(t)}$ fixed, and perform backtracking to ensure 
            \State \hspace{0.5cm} $\mathcal{L}$ does not increase.
            \State \textbf{($\beta$-update)}
            \State \hspace{0.5cm} Update $\beta^{(t+1)}$ from $\beta^{(t)}$ using a Newton-like step for the Cox loss
            \State \hspace{0.5cm} (Eq.~\ref{eqn:beta_update}), holding $W^{(t+1)}$ and $H^{(t+1)}$ fixed.
            \State $lossNew = \mathcal{L}(W^{(t+1)},H^{(t+1)},\beta^{(t+1)})$
            \State $eps = |lossNew - loss| / |loss|$
            \State $loss = lossNew$
            \State $t = t + 1$
        \EndWhile
        \State \Return $\hat{W} = W^{(t+1)}, \hat{H} = H^{(t+1)}, \hat{\beta}=\beta^{(t+1)}$
    \end{algorithmic}
\end{algorithm}

\subsubsection{Update for $H$}
Conditional on $W$, the loss $\mathcal{L}(W,H,\beta)$ reduces to a strictly
convex quadratic function of $H$. We adopt the standard NMF multiplicative
update with $\ell_2$ penalty:
\begin{equation}
\label{eqn:Hupdate}
H \leftarrow
\max\!\left(
  H \odot \frac{W^\top X}{
    W^\top W H + \lambda_H H + \varepsilon_H
  },
  \varepsilon_H
\right),
\end{equation}
where $\odot$ denotes elementwise multiplication, all divisions are
elementwise, and $\varepsilon_H > 0$ is a small floor to prevent entries from
becoming exactly zero. This update is equivalent to a majorization–minimization
step and guarantees a nonincreasing reconstruction term conditional on $W$
[@seung2001algorithms; @pascualmontano2006nonsmooth]. The $\varepsilon_H$-floor
ensures that iterates remain in the interior of the nonnegative orthant, which
simplifies the convergence analysis.

  
\subsubsection{Update for $W$}
For $W$, we construct a hybrid multiplicative update that balances the
contributions of the NMF and Cox gradients. Let
\[
\nabla_W \mathcal{L}_{\mathrm{NMF}}(W,H)
=
\frac{1}{np}(W H - X)H^\top,
\]
and let
\[
\nabla_W \mathcal{L}_{\mathrm{Cox}}(W,\beta)
=
\frac{2}{n_{event}}\nabla_W \ell(W,\beta).
\]
where $\nabla_W \ell(W,\beta)$ denotes the Cox gradient with respect to $W$. A closed-form expression for $\nabla_W \ell$ is
\begin{equation}
\nabla_W \ell(W,\beta)
=
\sum_{i=1}^n
\delta_i
\left(
  x_i
  -\frac{
      \sum_{j : y_j \ge y_i}
        x_j \exp(x_j^\top W \beta)
    }{
      \sum_{j : y_j \ge y_i}
        \exp(x_j^\top W \beta)
    }
\right)\beta^\top,
\end{equation}
where $x_i$ denotes the $i$th column of $X$.

We define the gradient-balancing factor
\begin{equation}
\label{eq:supp-delta}
\delta^{(t)}
=
\frac{
  \left\lVert (1-\alpha) \nabla_W \mathcal{L}_{\mathrm{NMF}}(W^{(t)},H^{(t+1)}) \right\rVert_F^2
}{
  \left\lVert \alpha \nabla_W \mathcal{L}_{Cox}(W^{(t)},\beta^{(t)}) \right\rVert_F^2
},
\end{equation}
and clip $\delta^{(t)}$ to avoid extreme values:
$\delta^{(t)} \leftarrow \min(\delta^{(t)}, \delta_{\max})$
with $\delta_{\max} = 10^6$ in all experiments.

The multiplicative factor for $W$ is then
\begin{equation}
R^{(t)}
=
\frac{
  \frac{(1-\alpha)}{np} X H^{(t+1)\top}
  +\frac{2\alpha}{n_{\mathrm{event}}}\,\delta^{(t)}\,\nabla_W \ell(W^{(t)},\beta^{(t)})
}{
  \frac{(1-\alpha)}{np} W^{(t)} H^{(t+1)}H^{(t+1)\top}
  +\varepsilon_W
},
\end{equation}
and the proposed update is
\begin{equation}
\label{eqn:W_update}
W^{(t+1)}
=
\max\!\left(
  W^{(t)} \odot R^{(t)},
  \varepsilon_W
\right),
\end{equation}
with a small floor $\varepsilon_W > 0$. When $\alpha=0$, this reduces
to the standard multiplicative update for NMF [@seung2001algorithms];
for $\alpha>0$, the Cox gradient perturbs the update in a direction that
decreases the supervised loss.

Because the combined multiplicative step does not, by itself, guarantee a
nonincrease in the full loss $\mathcal{L}$, we embed it in a backtracking
line search.

\begin{algorithm}[H]
\caption{$W$ update with backtracking}
\label{alg:backtrack}
    \begin{algorithmic}[1]
        \Require Value of $W$ and the previous iteration $t$: $W^{(t)}$, the multiplicative update term at the current iteration $R^{(t+1)}$, the max number of backtracking updates $max\_bt$, the backtracking parameter $\rho \subset (0,1)$
        \Ensure $W^{(t+1)}$
        \State $\theta =1$
        \State b = 1
        \State $flag\_accept = FALSE$
        \While{$b \leq max\_bt$}
            \State $W^{(t+1)}_{cand} = W^{(t)} \odot [(1-\theta) + \theta R^{(t+1)}]$
            \State \textbf{(Column normalization)}
            \State \hspace{0.5cm} Compute $D = \mathrm{diag}\!\big(\lVert W^{(t+1)}_{(:,1)cand}\rVert_2,\dots,\lVert W^{(t+1)}_{(:,k)cand}\rVert_2\big)$
            \State \hspace{0.5cm} and set $W^{(t+1)}_{cand} \leftarrow W^{(t+1)}_{cand} D^{-1}$, $H^{(t+1)}_{cand} \leftarrow D H^{(t+1)}$, and $\beta^{(t)}_{cand} \leftarrow D \beta^{(t)}$
            \If{$\mathcal{L}(W^{(t+1)}_{cand},H^{(t+1)},\beta^{(t)}) \leq \mathcal{L}(W^{(t)},H^{(t+1)},\beta^{(t)})$}
              \State $W^{(t+1)} = W^{(t+1)}_{cand}$
              \State $H^{(t+1)} = H^{(t+1)}_{cand}$
              \State $\beta^{(t)} = \beta^{(t)}_{cand}$
              \State $flag\_accept = TRUE$
              \State break
            \EndIf
            \State $\theta = \theta*\rho$
            \State $b = b + 1$
        \EndWhile
        \If{$flag\_accept = FALSE$}
            \State $W^{(t+1)}=W^{(t)}$
        \EndIf
    \end{algorithmic}
\end{algorithm}

The column normalization preserves both $WH$ and $W\beta$, and therefore
leaves the loss $\mathcal{L}$ invariant up to numerical error. Backtracking
guarantees that the accepted $W$ update does not increase $\mathcal{L}$.

\subsubsection{Update for $\beta$} 

Conditional on $(W,H)$, the loss in $\beta$ reduces to a convex
elastic-net–penalized Cox problem:
\[
\min_{\beta \in \mathbb{R}^k}
\left\{
  -\frac{2\alpha}{n_{\mathrm{event}}}\,\ell(W,\beta)
  +
  \lambda\left(
    \xi \lVert \beta \rVert_1
    +\frac{1-\xi}{2} \lVert \beta \rVert_2^2
  \right)
\right\}.
\]
We solve this subproblem by cyclic coordinate descent following
[@simon2011regularization]. Writing $\ell(\beta) = \ell(W,\beta)$ and
$\tilde{\eta}_i = Z_i^\top \beta$, the update for coordinate $r$ has the
closed form
\begin{equation}
\label{eqn:beta_update}
\hat{\beta}_r
=
\frac{
  S\!\left(
    \dfrac{1}{n}
    \sum_{i=1}^n w(\tilde{\eta})_i
    z_{i,r}
    \left[
      v(\tilde{\eta})_i - \sum_{j \ne r} z_{i,j} \beta_j
    \right],
    \lambda \xi
  \right)
}{
  \frac{1}{n} \sum_{i=1}^n w(\tilde{\eta})_i z_{i,r}^2 + \lambda(1-\xi)
},
\end{equation}
where $S(a,\tau) = \mathrm{sign}(a)\max(|a|-\tau,0)$ is the soft-thresholding
operator, and $w(\tilde{\eta})$, $v(\tilde{\eta})$ are standard local
quadratic approximations of the Cox log-partial likelihood [@simon2011regularization].
We iterate coordinate updates until the subproblem converges to numerical
tolerance, yielding a minimizer in $\beta$ for the current $W$.


\subsubsection{Normalization of W}

After each accepted $W$ update, we normalize the columns of $W$ as
$W \leftarrow W D^{-1}$, and adjust $H$ and $\beta$ as
\[
H \leftarrow D H, \qquad \beta \leftarrow D \beta,
\]
where $D$ is the diagonal matrix of column $\ell_2$-norms of $W$.
This preserves the reconstruction $WH$ and the linear predictor $W\beta$,
leaving both the NMF and Cox terms in the loss unchanged. Normalization
prevents degeneracy in the scale-nonidentifiable factorization and keeps
columns of $W$ comparable in magnitude and interpretable as gene programs.


\subsubsection{Derivation of W update from projected coordinate descent}
The W update can be derived directly from the projected coordinate descent update with a specific step size.

Recall that the overall loss function is
\begin{align}
    \mathcal{L}(W,H,\beta) = \frac{(1-\alpha)}{2np} ||X - WH||^2_F - \frac{2 \alpha}{n_{event}} \ell(W,\beta) + \lambda(\xi||\beta||_1 + \frac{(1-\xi)}{2},
\end{align}
where $\ell(W,\beta)$ is the log-partial likelihood for the Cox model. Let $\nabla_W \ell$ represent the derivative of $\ell(W,\beta)$ with respect to $W$. Then the derivative of the overall loss with respect to $W$ is

\begin{equation}
  \frac{\partial L}{\partial W} = \frac{(1-\alpha)}{np}\left( WHH^T - XH^T \right) - \frac{2\alpha}{n_{event}} \nabla_W \ell
\end{equation}

Then the gradient descent update rule at iteration $t$ is
\begin{align}
  W^{(t)} &= W^{(t-1)} - \gamma \left( \frac{\partial L}{\partial W} \right) \nonumber \\
          &= W^{(t-1)} - \gamma \left( \frac{(1-\alpha)}{np}\left( WHH^T - XH^T \right) - \frac{2\alpha}{n_{event}} \nabla_W \ell \right)\\
\end{align}
Let the step size $\gamma$ be defined as in CITE:
\begin{equation}
  \gamma = \frac{np}{(1-\alpha)}\frac{W}{WHH^T}
\end{equation}
Then the update becomes
\begin{align}
  W^{(t)} &= W^{(t-1)} -\frac{np}{(1-\alpha)}\frac{W}{WHH^T} \left( \frac{(1-\alpha)}{np}\left( WHH^T - XH^T \right) - \frac{2\alpha}{n_{event}} \nabla_W \ell \right) \nonumber \\
          &= \frac{W}{WHH^T} XH^T + \frac{2\alpha np}{n_{event}(1-\alpha)} \nabla_W \ell \nonumber \\
          &= W \odot \frac{XH^T + \frac{2\alpha np}{n_{event}(1-\alpha)} \nabla_W \ell }{WHH^T} \nonumber \\
          &= W \odot \frac{\frac{(1-\alpha)}{np} XH^T + \frac{2\alpha}{n_{event}} \nabla_W \ell}{\frac{(1-\alpha)}{np} WHH^T}
\end{align}
Finally, projected coordinate descent projects the $W$ update in to the positive space
\begin{equation}
  W^{(t)} =  \max \left(W \odot \frac{\frac{(1-\alpha)}{np} XH^T + \frac{2\alpha}{n_{event}} \nabla_W \ell}{\frac{(1-\alpha)}{np}WHH^T}, 0\right)
\end{equation}
Since $W \geq 0$ this is equivalent to
\begin{equation}
  W^{(t)} =  W \odot \max \left( \frac{\frac{(1-\alpha)}{np} XH^T + \frac{2\alpha}{n_{event}} \nabla_W \ell}{\frac{(1-\alpha)}{np} WHH^T}, 0 \right)
\end{equation}
which matches the multiplicative form used in the software implementation.


\subsection{Convergence proof}

We show that, under mild regularity conditions, the block coordinate descent
(BCD) Algorithm~S\ref{alg:desurv} converges to a stationary point of the DeSurv
loss function
\[
\mathcal{L}(W,H,\beta).
\]
For clarity, we first analyze the algorithm \emph{without} the column
normalization of $W$ inside the loop, and then argue in
Section~\ref{subsec:conv_normalization} that the normalization step preserves
stationarity of limit points.

Throughout, let $\theta = (W,H,\beta)$ and denote
\(\mathcal{L}(\theta) = \mathcal{L}(W,H,\beta)\).
The feasible set is
\[
\Theta = \{(W,H,\beta):
  W \in \mathbb{R}_{\ge 0}^{p\times k},\;
  H \in \mathbb{R}_{\ge 0}^{k\times n},\;
  \beta \in \mathbb{R}^k\}.
\]

\textbf{Assumption 1 (Regularity and parameter space).}
We assume:
\begin{enumerate}[label=(\roman*)]
  \item The data $X \in \mathbb{R}_{\ge 0}^{p\times n}$, $y \in \mathbb{R}^n$, and $\delta \in \{0,1\}^n$ are fixed and bounded.
  \item The hyperparameters satisfy $\lambda_H > 0$, $\lambda>0$, $\alpha \in [0,1)$, and $\xi \in [0,1]$.
  \item The initial iterate $\theta^{(0)} = (W^{(0)},H^{(0)},\beta^{(0)})$ lies in $\Theta$ and satisfies $W^{(0)},H^{(0)} > 0$ elementwise.
\end{enumerate}

\textbf{Assumption 2 (Block updates).}
At each outer iteration $t$, the three block updates in
Algorithm~S\ref{alg:desurv} satisfy:
\begin{enumerate}[label=(\roman*)]
  \item \emph{$H$-update.} For fixed $(W^{(t)},\beta^{(t)})$, the update
        $H^{(t)} \mapsto H^{(t+1)}$ is given by the multiplicative rule
        \[
        H \leftarrow H \odot \frac{W^\top X}{W^\top W H + \lambda_H H},
        \]
        applied at $(W^{(t)},H^{(t)})$. This rule preserves nonnegativity and
        yields a nonincreasing value of the reconstruction term conditional on
        $W^{(t)}$ and $\beta^{(t)}$; see e.g.
        [@seung2001algorithms; @pascualmontano2006nonsmooth; @lin2007convergence].
  \item \emph{$W$-update.} For fixed $(H^{(t+1)},\beta^{(t)})$, the update
        $W^{(t)} \mapsto W^{(t+1)}$ is the hybrid multiplicative step
        followed by backtracking as in Algorithm~S\ref{alg:backtrack}, producing
        $W^{(t+1)}$ such that
        \[
          \mathcal{L}(W^{(t+1)},H^{(t+1)},\beta^{(t)})
          \le
          \mathcal{L}(W^{(t)},H^{(t+1)},\beta^{(t)}).
        \]
        If no candidate satisfies the Armijo-type condition within the allowed
        number of backtracking steps, the algorithm sets
        $W^{(t+1)} := W^{(t)}$.
  \item \emph{$\beta$-update.} For fixed $(W^{(t+1)},H^{(t+1)})$, the update
        $\beta^{(t)} \mapsto \beta^{(t+1)}$ is obtained by running the
        coordinate descent method of [@simon2011regularization] on the convex elastic-net Cox
        subproblem until convergence. Thus
        \[
          \beta^{(t+1)} \in
          \arg\min_{\beta\in\mathbb{R}^k}
          \mathcal{L}(W^{(t+1)},H^{(t+1)},\beta),
        \]
        and
        \[
          \mathcal{L}(W^{(t+1)},H^{(t+1)},\beta^{(t+1)})
          \le
          \mathcal{L}(W^{(t+1)},H^{(t+1)},\beta^{(t)}).
        \]
\end{enumerate}

\vspace{0.5em}
\textbf{Lemma 1 (Continuity and bounded level sets).}
Under Assumption 1, $\mathcal{L}:\Theta\to\mathbb{R}$ is continuous, and the
initial sublevel set
\[
  \mathcal{S}_0 := \{\theta \in \Theta:
    \mathcal{L}(\theta) \le \mathcal{L}(\theta^{(0)})\}
\]
is nonempty, closed, and bounded.

\emph{Proof.}
The NMF term $\|X-WH\|_F^2$ is a polynomial in the entries of $W$ and $H$, hence
is continuously differentiable. The penalty $\|H\|_F^2$ is continuous as well.
The Cox partial log-likelihood is a smooth function of the linear predictor
$\eta_i = x_i^\top W \beta$, which is linear in $(W,\beta)$, so this term is
continuously differentiable in $(W,\beta)$. The $\ell_2$ penalty on $\beta$ is
smooth, and the $\ell_1$ penalty is convex and lower semicontinuous. Therefore
$\mathcal{L}$ is continuous on $\Theta$.

The constraints $W,H\ge 0$ define closed convex cones, and
$\beta\in\mathbb{R}^k$ is unconstrained. Because $\lambda_H>0$ and $\lambda>0$,
the terms $\tfrac{\lambda_H}{nk}\|H\|_F^2$ and
$\lambda\tfrac{1-\xi}{2}\|\beta\|_2^2$ dominate the objective as
$\|H\|_F\to\infty$ or $\|\beta\|_2\to\infty$, respectively. Thus the sublevel
set $\mathcal{S}_0$ is bounded in $(H,\beta)$. Since $W\ge 0$ and $X$ is
fixed, the reconstruction term and Cox term being bounded prevent $W$ from
diverging while $\mathcal{L}$ remains below $\mathcal{L}(\theta^{(0)})$, so
$W$ is bounded on $\mathcal{S}_0$. Because $\Theta$ is closed and
$\mathcal{L}$ is continuous, $\mathcal{S}_0$ is closed. Nonemptiness follows
from $\theta^{(0)}\in\mathcal{S}_0$. \hfill$\square$

\vspace{0.5em}
\textbf{Lemma 2 (Monotone descent and existence of limit points).}
Under Assumptions 1 and 2, Algorithm~S\ref{alg:desurv} (without $W$
normalization) generates a sequence $\{\theta^{(t)}\}_{t\ge 0}\subset\mathcal{S}_0$
such that
\[
  \mathcal{L}(\theta^{(t+1)}) \le \mathcal{L}(\theta^{(t)})\quad \forall\,t,
\]
and $\{\mathcal{L}(\theta^{(t)})\}$ converges to a finite limit $\mathcal{L}^*$.
Moreover, $\{\theta^{(t)}\}$ is bounded and therefore admits at least one limit
point.

\emph{Proof.}
By Assumption 2(i),
\[
  \mathcal{L}(W^{(t)},H^{(t+1)},\beta^{(t)})
  \le
  \mathcal{L}(W^{(t)},H^{(t)},\beta^{(t)}).
\]
By Assumption 2(ii),
\[
  \mathcal{L}(W^{(t+1)},H^{(t+1)},\beta^{(t)})
  \le
  \mathcal{L}(W^{(t)},H^{(t+1)},\beta^{(t)}).
\]
By Assumption 2(iii),
\[
  \mathcal{L}(W^{(t+1)},H^{(t+1)},\beta^{(t+1)})
  \le
  \mathcal{L}(W^{(t+1)},H^{(t+1)},\beta^{(t)}).
\]
Combining these inequalities gives
\[
  \mathcal{L}(\theta^{(t+1)}) \le \mathcal{L}(\theta^{(t)})\quad\forall t,
\]
so $\{\mathcal{L}(\theta^{(t)})\}$ is monotonically nonincreasing. Since
$\mathcal{L}$ is bounded below on $\Theta$, the sequence converges to some
finite $\mathcal{L}^*$.

Each iterate lies in $\mathcal{S}_0$ by construction, and $\mathcal{S}_0$ is
bounded by Lemma 1. Therefore $\{\theta^{(t)}\}$ is bounded and has at least
one limit point by the Bolzano--Weierstrass theorem. \hfill$\square$

\vspace{0.5em}
\textbf{Lemma 3 (Blockwise optimality of limit points).}
Let $\theta^*=(W^*,H^*,\beta^*)$ be any limit point of $\{\theta^{(t)}\}$ under
Assumptions~1–2. Then:
\begin{enumerate}[label=(\roman*)]
  \item $H^*$ satisfies the KKT conditions for
        \[
          \min_{H\ge 0} \mathcal{L}(W^*,H,\beta^*).
        \]
  \item $W^*$ satisfies the KKT conditions for
        \[
          \min_{W\ge 0} \mathcal{L}(W,H^*,\beta^*).
        \]
  \item $\beta^*$ is the unique minimizer of
        \[
          \min_{\beta\in\mathbb{R}^k} \mathcal{L}(W^*,H^*,\beta),
        \]
        and satisfies the KKT conditions for the elastic-net Cox subproblem.
\end{enumerate}

\emph{Proof.}
Let $\{t_j\}$ be a subsequence such that
$\theta^{(t_j)}\to\theta^*=(W^*,H^*,\beta^*)$. By continuity of $\mathcal{L}$,
$\mathcal{L}(\theta^{(t_j)})\to\mathcal{L}^*$.

(i) Conditional on $(W,\beta)$, the $H$-subproblem is
\begin{equation}
  \min_{H \geq 0}\frac{(1-\alpha)}{np}(\lVert X-WH \rVert_F^2 + \frac{\lambda_H}{nk}||H||_F^2) + \text{const in $H$},\nonumber
\end{equation}
a strictly convex quadratic program. The multiplicative update for $H$ is known
to be a descent method whose fixed points coincide with the KKT points of this
constrained subproblem [@seung2001algorithms; @lin2007convergence]. Since the full
objective $\mathcal{L}(\theta^{(t)})$ converges, the per-iteration decrease
in $\mathcal{L}$ due to the $H$-update must vanish along $\{t_j\}$.
If $H^*$ were not KKT for the $H$-subproblem given $(W^*,\beta^*)$, there would
exist a feasible descent direction for $H$ at $(W^*,\beta^*)$, implying that
for sufficiently large $j$ the $H$-update would still produce a strict decrease
in $\mathcal{L}$, contradicting convergence. Hence $H^*$ satisfies the KKT
conditions.

(ii) Conditional on $(H,\beta)$, the $W$-subproblem is differentiable and
convex in $W$ (sum of a quadratic term and a negative Cox partial
log-likelihood in a linear predictor). The hybrid multiplicative step with
backtracking can be viewed as a projected gradient-like update onto the
nonnegative orthant. Under mild conditions on the search direction and step
sizes, any limit point of such a projected gradient scheme is a KKT point for
the constrained subproblem; see, for example, [@tseng2001convergence; @grippo2000convergence].
If $W^*$ did not satisfy the KKT conditions, there would be a feasible
descent direction at $(W^*,H^*,\beta^*)$ and a sufficiently small step that
reduces $\mathcal{L}$, which the line search would eventually accept. This
would contradict the fact that $\mathcal{L}(\theta^{(t_j)})\to\mathcal{L}^*$.
Therefore $W^*$ is KKT for the $W$-subproblem.

(iii) For fixed $(W,H)$, the $\beta$-subproblem is the convex elastic-net
penalized Cox objective. The coordinate descent algorithm converges to the unique minimizer. By Assumption 2(iii),$\beta^{(t+1)}$ is taken to be this minimizer at each iteration. Passing to the limit along $\{t_j\}$ shows that $\beta^*$ is the unique minimizer of the
$\beta$-block given $(W^*,H^*)$ and hence satisfies its KKT conditions.
\hfill$\square$

\vspace{0.5em}
\textbf{Theorem 1 (Convergence to a stationary point).}
Under Assumptions 1 and 2, the sequence $\{\theta^{(t)}\}$ produced by
Algorithm~S\ref{alg:desurv} (without $W$ normalization) satisfies:
\begin{enumerate}[label=(\alph*)]
  \item The objective values $\{\mathcal{L}(\theta^{(t)})\}$ are monotonically
        nonincreasing and converge to a finite limit $\mathcal{L}^*$.
  \item Every limit point $\theta^* = (W^*,H^*,\beta^*)$ of $\{\theta^{(t)}\}$
        is a stationary point of $\mathcal{L}$ on $\Theta$, i.e. it satisfies
        the first-order KKT conditions for
        \[
          \min_{\theta\in\Theta} \mathcal{L}(\theta).
        \]
\end{enumerate}

\emph{Proof.}
Part (a) is Lemma 2. For part (b), Lemma 3 shows that any limit point $\theta^*$
is blockwise optimal: each block is optimal (in the KKT sense) when the others
are held fixed.

The DeSurv loss can be written as
\[
\mathcal{L}(\theta) = f(W,H,\beta) + g(\beta)
                     + I_{\{W\ge 0\}}(W) + I_{\{H\ge 0\}}(H),
\]
where $f$ is continuously differentiable, $g(\beta)=\lambda\xi\|\beta\|_1$ is
a separable convex (possibly nondifferentiable) penalty, and $I_C$ denotes the
indicator of a closed convex set $C$. This is the smooth+nonsmooth composite
form considered in block coordinate descent analyses such as [@tseng2001convergence].
In this setting, any point that is optimal with respect to each block
individually (a block coordinatewise minimizer) is a stationary point for the
full problem: equivalently,
\[
0 \in \nabla_{W,H} f(W^*,H^*,\beta^*)
    + \partial I_{\{W\ge 0\}}(W^*)
    + \partial I_{\{H\ge 0\}}(H^*),
\]
\[
0 \in \nabla_\beta f(W^*,H^*,\beta^*)
    + \partial g(\beta^*),
\]
which are precisely the KKT conditions for
$\min_{\theta\in\Theta}\mathcal{L}(\theta)$.
Hence every limit point of Algorithm~S\ref{alg:desurv} is stationary.
\hfill$\square$

\subsubsection{Effect of column normalization of $W$}
\label{subsec:conv_normalization}

The above analysis omits the column-normalization step for $W$ used in
Algorithm~S\ref{alg:backtrack}. We briefly argue that this normalization does
not affect stationarity of limit points.

Let $D$ be a diagonal matrix with strictly positive diagonal entries. The
transformation
\[
  (W,H,\beta) \mapsto (W',H',\beta') = (W D^{-1},\, D H,\, D\beta)
\]
preserves both the product $WH$ and the linear predictor $W\beta$, and hence
leaves $\mathcal{L}$ invariant. The column-normalization step is exactly such a
transformation, with $D$ chosen from the column norms of $W$.

Let $\{\theta^{(t)}\}$ be the sequence generated by the algorithm without
normalization, and let $\{\tilde{\theta}^{(t)}\}$ be the sequence with
normalization. For each $t$ there exists a diagonal $D^{(t)}$ with strictly
positive entries such that
\[
  \tilde{\theta}^{(t)} =
  (W^{(t)} D^{(t),-1},\, D^{(t)} H^{(t)},\, D^{(t)}\beta^{(t)}),
\]
and $\mathcal{L}(\tilde{\theta}^{(t)}) = \mathcal{L}(\theta^{(t)})$.
Thus limit points of $\{\tilde{\theta}^{(t)}\}$ are obtained from limit points
of $\{\theta^{(t)}\}$ by such invertible diagonal scalings.

Since the KKT conditions are expressed in terms of the gradients with respect
to $WH$ and $W\beta$, and these quantities are invariant under the above
scaling, stationarity is preserved under the transformation. Therefore
Theorem~1 implies that every limit point of the \emph{normalized} algorithm is
also a stationary point of $\mathcal{L}$ (up to this scaling equivalence), which
establishes convergence of the implementation used in practice.

\subsubsection{Remark (Coxnet implementation)} 
Our implementation updates the $\beta$ block using a Coxnet-style coordinate descent that relies on an approximate Hessian. Consequently, the formal optimality proof is established for an idealized version of the $\beta$ update that assumes exact second-order information. Nonetheless, in empirical applications the approximate update is numerically stable and yields a monotone decrease in the objective, consistent with the behavior predicted by the idealized analysis. 


\subsection{Supervision on \(W\) versus \(H\)}

Classical nonnegative matrix factorization (NMF) is non-identifiable: for any
invertible matrix \(R\) with nonnegative entries, the factorization
\((W, H)\) can be transformed to \((W R, R^{-1} H)\) without changing the
reconstruction error, yielding multiple valid solutions
[@donoho2004nmf; @laurberg2008uniqueness; @gillis2014nmf]. Although normalizing
columns of \(W\) or rows of \(H\) resolves the trivial scaling ambiguity,
nonnegative mixing transformations persist, and empirical studies consistently
show that the sample-loading matrix \(H\) is more sensitive to initialization
and local minima than the program matrix \(W\)
[@brunet2004metagenes; @kim2007sparse; @gillis2012accelerated].

In DeSurv, survival supervision enters through the projection $Z = X^\top W$ in the partial log-likelihood, so that the gradient of the DeSurv loss with respect to the programs is
\[
\nabla \mathcal{L} = (1-\alpha) \nabla_W \mathcal{L}_{\mathrm{Cox}}(W,H) - \alpha\nabla_W \mathcal{L}_{\mathrm{Cox}}(W,\beta),
\]
and therefore the update rule for $W$ as in Equation~\ref{eqn:W_update} relies on both the reconstruction term and the supervision term. As such, the gene-program definitions are directly shaped by their association with survival. This mechanism amplifies
genes aligned with the hazard gradient and suppresses those that dilute
prognostic structure, ensuring that the learned programs encode survival-relevant
biological variation.

By contrast, if supervision were applied to the sample loadings \(H\), the model
could reduce the Cox loss by redistributing patient-specific coefficients while
leaving the gene-level programs \(W\) nearly unchanged, a behavior documented in
multiple supervised variants of NMF and supervised topic models, where
supervision applied only to the coefficient matrix modifies \(H\) but not the
basis \(W\) [@cai2011graph; @wang2014supervised; @blei2007slda].

Supervising \(W\) also provides a practical advantage for **portability**:
because \(W\) defines gene-level programs, new samples can be embedded by the
closed-form projection \(Z_{\text{new}} = X_{\text{new}}^\top W\). In contrast,
supervising \(H\) would not yield such a mapping; instead, obtaining sample
loadings for external datasets would require solving a nonnegative least-squares
(NNLS) problem for each new sample, introducing optimization noise and reducing
reproducibility.

Together, these considerations motivate supervising through \(W\), which shapes
the gene programs toward prognostic directions, stabilizes solutions across
initializations, and yields biologically interpretable signatures that
generalize across cohorts.


\subsection{Cross-validation procedure}
Algorithm~S\ref{alg:cv} describes the cross validation procedure with $F$ folds and $R$ initializations.
We define the c-index using comparable pairs
$\mathcal{P} = \{(i,j) : y_i < y_j,\ \delta_i = 1\}$ and linear predictors
$\hat{\eta}_i$:
\begin{equation}
\label{eqn:cindex}
\hat{c} =
\frac{1}{|\mathcal{P}|}
\sum_{(i,j)\in\mathcal{P}}
\left[
  \mathbb{1}(\hat{\eta}_i > \hat{\eta}_j)
  + \tfrac{1}{2}\mathbb{1}(\hat{\eta}_i = \hat{\eta}_j)
\right].
\end{equation}

\begin{algorithm}[H]
\caption{Cross-validation for DeSurv pipeline}
\label{alg:cv}
\begin{algorithmic}[1]
\Require 
\Statex Number of folds $F$
\Statex Number of initializations $R$
\Statex Observed data ($X$, $y$, $\delta$)
\Statex Hyperparameters $k$, $\alpha$, $\lambda$, $\xi$, and $\lambda_H$
\Ensure The cross-validated c-index

\State Divide subjects into $F$ folds.
\For{$f = 1$ to $F$}
    \State Split data into training and validation $(X_{(-f)},y_{(-f)}, \delta_{(-f)})$, and $(X_{(f)},y_{(f)}, \delta_{(f)})$ sets
    \For{$r = 1$ to $R$}
        \State set.seed$(r)$
        \State Apply Algorithm~\ref{alg:desurv} with inputs $(X_{(-f)},y_{(-f)}, \delta_{(-f)})$, and $(k, \alpha, \lambda, \xi, \lambda_H)$
        \State Obtain $\hat{W}$ and $\hat{\beta}$ as output from Algorithm~\ref{alg:desurv}
        \State Compute the estimated linear predictor: 
        $
        \hat{\eta} = X_{(f)}^T\hat{W}\hat{\beta}
        $
        \State Compute c-index, denoted $\hat{c}_{(f)r}$, according to Equation~\ref{eqn:cindex}
    \EndFor
    \State \textbf{end loop over $r$}
    \State Compute average c-index across initializations:
    $
    \hat{c}_{(f)} = \frac{1}{r}\sum_{r=1}^R\hat{c}_{(f)r}
    $
\EndFor
\State \textbf{end loop over $f$}
\State Compute final cross-validated c-index:
$
\hat{c} = \frac{1}{F}\sum_{f=1}^F \hat{c}_{(f)}
$
\State \Return Final estimate $\hat{c}$

\end{algorithmic}
\end{algorithm}


\subsection{Bayesian Optimization}

\subsection{PDAC Datasets}

\subsection{Bladder Datasets}

\subsection{Consensus Clustering}

\subsection{Survival Analysis}

\pnasbreak

# Versioning {.unnumbered}

\end{verbatim}

\subsection{DeSurv package version:
1.0.1}\label{desurv-package-version-1.0.1}

\subsection{DeSurv git branch:
20260107bugfix}\label{desurv-git-branch-20260107bugfix}

\subsection{DeSurv git commit:
fea641a96e4743315a35b9b6addcc1a1ff53da9d}\label{desurv-git-commit-fea641a96e4743315a35b9b6addcc1a1ff53da9d}

\subsection{Paper git branch: main}\label{paper-git-branch-main}

\subsection{Paper git commit:
524e354dd777e74a98130a69b6296cca4eefbd51}\label{paper-git-commit-524e354dd777e74a98130a69b6296cca4eefbd51}

```

\phantomsection\label{refs}
\begin{CSLReferences}{0}{1}
\bibitem[\citeproctext]{ref-pareja2016triple}
\CSLLeftMargin{1. }%
\CSLRightInline{Pareja F, et al. (2016) Triple-negative breast cancer:
The importance of molecular and histologic subtyping, and recognition of
low-grade variants. \emph{NPJ breast cancer} 2(1):1--11.}

\bibitem[\citeproctext]{ref-dienstmann2018molecular}
\CSLLeftMargin{2. }%
\CSLRightInline{Dienstmann R, Salazar R, Tabernero J (2018) Molecular
subtypes and the evolution of treatment decisions in metastatic
colorectal cancer. \emph{Am Soc Clin Oncol Educ Book} 38(38):231--8.}

\bibitem[\citeproctext]{ref-zhou2021clinical}
\CSLLeftMargin{3. }%
\CSLRightInline{Zhou X, et al. (2021) Clinical impact of molecular
subtyping of pancreatic cancer. \emph{Frontiers in cell and
developmental biology} 9:743908.}

\bibitem[\citeproctext]{ref-seiler2017impact}
\CSLLeftMargin{4. }%
\CSLRightInline{Seiler R, et al. (2017) Impact of molecular subtypes in
muscle-invasive bladder cancer on predicting response and survival after
neoadjuvant chemotherapy. \emph{European urology} 72(4):544--554.}

\bibitem[\citeproctext]{ref-prat2015clinical}
\CSLLeftMargin{5. }%
\CSLRightInline{Prat A, et al. (2015) Clinical implications of the
intrinsic molecular subtypes of breast cancer. \emph{The Breast}
24:S26--S35.}

\bibitem[\citeproctext]{ref-Ou2021}
\CSLLeftMargin{6. }%
\CSLRightInline{Ou F, Michiels S, Shyr Y, Adjei AA, Oberg AL (2021)
\href{https://doi.org/10.1016/j.jtho.2021.01.1616}{Biomarker discovery
and validation: Statistical considerations}. \emph{Journal of Thoracic
Oncology} 16(Suppl 15):S539--S547.}

\bibitem[\citeproctext]{ref-Planey2016}
\CSLLeftMargin{7. }%
\CSLRightInline{Planey Catherine\,R, Gevaert O (2016)
\href{https://doi.org/10.1186/s13073-016-0281-4}{CoINcIDE: A framework
for discovery of patient subtypes across multiple datasets}.
\emph{Genome Medicine} 8(1):27.}

\bibitem[\citeproctext]{ref-Prat2014}
\CSLLeftMargin{8. }%
\CSLRightInline{Prat A, Pineda E, Adamo B, et\,al. (2014)
\href{https://doi.org/10.1093/jnci/dju152}{Molecular features and
survival outcomes of the intrinsic subtypes in the international breast
cancer study group trial 10‑93}.
\emph{Journal\,of\,the\,National\,Cancer\,Institute} 106(8):dju152.}

\bibitem[\citeproctext]{ref-ellrott2025classification}
\CSLLeftMargin{9. }%
\CSLRightInline{Ellrott K, et al. (2025) Classification of non-TCGA
cancer samples to TCGA molecular subtypes using compact feature sets.
\emph{Cancer cell} 43(2):195--212.}

\bibitem[\citeproctext]{ref-tomczak2015review}
\CSLLeftMargin{10. }%
\CSLRightInline{Tomczak K, Czerwińska P, Wiznerowicz M (2015) Review the
cancer genome atlas (TCGA): An immeasurable source of knowledge.
\emph{Contemporary Oncology/Wsp{ó}{ł}czesna Onkologia} 2015(1):68--77.}

\bibitem[\citeproctext]{ref-zhang2019international}
\CSLLeftMargin{11. }%
\CSLRightInline{Zhang J, et al. (2019) The international cancer genome
consortium data portal. \emph{Nature biotechnology} 37(4):367--369.}

\bibitem[\citeproctext]{ref-nguyen2024fourteen}
\CSLLeftMargin{12. }%
\CSLRightInline{Nguyen H, Nguyen H, Tran D, Draghici S, Nguyen T (2024)
Fourteen years of cellular deconvolution: Methodology, applications,
technical evaluation and outstanding challenges. \emph{Nucleic Acids
Research} 52(9):4761--4783.}

\bibitem[\citeproctext]{ref-lee1999learning}
\CSLLeftMargin{13. }%
\CSLRightInline{Lee DD, Seung HS (1999) Learning the parts of objects by
non-negative matrix factorization. \emph{nature} 401(6755):788--791.}

\bibitem[\citeproctext]{ref-Bailey2016}
\CSLLeftMargin{14. }%
\CSLRightInline{Bailey P, Chang DK, et al. (2016)
\href{https://doi.org/10.1038/nature16965}{Genomic analyses identify
molecular subtypes of pancreatic cancer}. \emph{Nature}
531(7592):47--52.}

\bibitem[\citeproctext]{ref-moffitt2015virtual}
\CSLLeftMargin{15. }%
\CSLRightInline{Moffitt RA, et al. (2015) Virtual microdissection
identifies distinct tumor-and stroma-specific subtypes of pancreatic
ductal adenocarcinoma. \emph{Nature genetics} 47(10):1168--1178.}

\bibitem[\citeproctext]{ref-peng2019novo}
\CSLLeftMargin{16. }%
\CSLRightInline{Peng XL, Moffitt RA, Torphy RJ, Volmar KE, Yeh JJ (2019)
De novo compartment deconvolution and weight estimation of tumor samples
using DECODER. \emph{Nature communications} 10(1):4729.}

\bibitem[\citeproctext]{ref-le2025survnmf}
\CSLLeftMargin{17. }%
\CSLRightInline{Le Goff V, et al. (2025) SurvNMF: Non-negative matrix
factorization supervised for survival data analysis. PhD thesis
(Institut Pasteur Paris; CEA).}

\bibitem[\citeproctext]{ref-huang2020low}
\CSLLeftMargin{18. }%
\CSLRightInline{Huang Z, Salama P, Shao W, Zhang J, Huang K (2020)
Low-rank reorganization via proportional hazards non-negative matrix
factorization unveils survival associated gene clusters. \emph{arXiv
preprint arXiv:200803776}.}

\end{CSLReferences}



% Bibliography
% \bibliography{pnas-sample}

\end{document}
